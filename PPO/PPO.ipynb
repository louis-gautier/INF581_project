{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HvFI3S6r8EZ"
      },
      "source": [
        "Source : https://github.com/nikhilbarhate99/PPO-PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4VJcUT2GlJz"
      },
      "source": [
        "################################################################################\n",
        "> # **Install Dependencies**\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rbpSQTflGlAr",
        "outputId": "f0bb898c-d4c6-4fcd-951e-0fdaf9cfc3a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting roboschool==1.0.48\n",
            "  Downloading roboschool-1.0.48-cp37-cp37m-manylinux1_x86_64.whl (44.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 44.9 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting gym==0.15.4\n",
            "  Downloading gym-0.15.4.tar.gz (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 65.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.21.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.15.0)\n",
            "Collecting pyglet<=1.3.2,>=1.2.0\n",
            "  Downloading pyglet-1.3.2-py2.py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 49.8 MB/s \n",
            "\u001b[?25hCollecting cloudpickle~=1.2.0\n",
            "  Downloading cloudpickle-1.2.2-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.4) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.15.4-py3-none-any.whl size=1648485 sha256=0216e6608f82407c58beb657f462525f1087aeaf77131012422017ffd46c617e\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/97/51/3adbfe67f40bce89b8eba2d3b8f42ec1c9f9c1e6305a73510d\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, cloudpickle, gym, roboschool\n",
            "  Attempting uninstall: pyglet\n",
            "    Found existing installation: pyglet 1.5.0\n",
            "    Uninstalling pyglet-1.5.0:\n",
            "      Successfully uninstalled pyglet-1.5.0\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.16.0 requires cloudpickle>=1.3, but you have cloudpickle 1.2.2 which is incompatible.\u001b[0m\n",
            "Successfully installed cloudpickle-1.2.2 gym-0.15.4 pyglet-1.3.2 roboschool-1.0.48\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cloudpickle"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting box2d-py\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 23.5 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |████████                        | 112 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 122 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 174 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 194 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 225 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 245 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 296 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 317 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 337 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 348 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 368 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 389 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 399 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 409 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 440 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 448 kB 12.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Collecting pybullet\n",
            "  Downloading pybullet-3.2.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (90.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 90.8 MB 332 bytes/s \n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-3.2.1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "############ install compatible version of OpenAI roboschool and gym ###########\n",
        "\n",
        "!pip install roboschool==1.0.48 gym==0.15.4\n",
        "\n",
        "!pip install box2d-py\n",
        "\n",
        "!pip install pybullet\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzZairIiGQ11"
      },
      "source": [
        "################################################################################\n",
        "> # **Introduction**\n",
        "> The notebook is divided into 5 major parts : \n",
        "\n",
        "*   **Part I** : define actor-critic network and PPO algorithm\n",
        "*   **Part II** : train PPO algorithm and save network weights and log files\n",
        "*   **Part III** : load (preTrained) network weights and test PPO algorithm\n",
        "*   **Part IV** : load log files and plot graphs\n",
        "*   **Part V** : install xvbf, load (preTrained) network weights and save images for gif and then generate gif\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s37cJXAYGrTY"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - I**\n",
        "\n",
        "*   define actor critic networks\n",
        "*   define PPO algorithm\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT6VUBg-F8Zm",
        "outputId": "29d3c7d0-28de-443e-ba2b-190c934b4003"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================================================\n",
            "Device set to : cpu\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "############################### Import libraries ###############################\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import MultivariateNormal\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "import pybullet_envs\n",
        "\n",
        "\n",
        "################################## set device ##################################\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "# set device to cpu or cuda\n",
        "device = torch.device('cpu')\n",
        "\n",
        "if(torch.cuda.is_available()): \n",
        "    device = torch.device('cuda:0') \n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
        "else:\n",
        "    print(\"Device set to : cpu\")\n",
        "    \n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################## PPO Policy ##################################\n",
        "\n",
        "\n",
        "class RolloutBuffer:\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.is_terminals = []\n",
        "    \n",
        "\n",
        "    def clear(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.is_terminals[:]\n",
        "\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        self.has_continuous_action_space = has_continuous_action_space\n",
        "\n",
        "        if has_continuous_action_space:\n",
        "            self.action_dim = action_dim\n",
        "            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
        "\n",
        "        # actor\n",
        "        if has_continuous_action_space :\n",
        "            self.actor = nn.Sequential(\n",
        "                            nn.Linear(state_dim, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, action_dim),\n",
        "                            nn.Tanh()\n",
        "                        )\n",
        "        else:\n",
        "            self.actor = nn.Sequential(\n",
        "                            nn.Linear(state_dim, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, action_dim),\n",
        "                            nn.Softmax(dim=-1)\n",
        "                        )\n",
        "\n",
        "        \n",
        "        # critic\n",
        "        self.critic = nn.Sequential(\n",
        "                        nn.Linear(state_dim, 64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, 64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, 1)\n",
        "                    )\n",
        "        \n",
        "    def set_action_std(self, new_action_std):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n",
        "        else:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"WARNING : Calling ActorCritic::set_action_std() on discrete action space policy\")\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "\n",
        "    def act(self, state):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            action_mean = self.actor(state)\n",
        "            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
        "            dist = MultivariateNormal(action_mean, cov_mat)\n",
        "        else:\n",
        "            action_probs = self.actor(state)\n",
        "            dist = Categorical(action_probs)\n",
        "\n",
        "        action = dist.sample()\n",
        "        action_logprob = dist.log_prob(action)\n",
        "        \n",
        "        return action.detach(), action_logprob.detach()\n",
        "    \n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            action_mean = self.actor(state)\n",
        "            action_var = self.action_var.expand_as(action_mean)\n",
        "            cov_mat = torch.diag_embed(action_var).to(device)\n",
        "            dist = MultivariateNormal(action_mean, cov_mat)\n",
        "            \n",
        "            # for single action continuous environments\n",
        "            if self.action_dim == 1:\n",
        "                action = action.reshape(-1, self.action_dim)\n",
        "\n",
        "        else:\n",
        "            action_probs = self.actor(state)\n",
        "            dist = Categorical(action_probs)\n",
        "\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        state_values = self.critic(state)\n",
        "        \n",
        "        return action_logprobs, state_values, dist_entropy\n",
        "\n",
        "\n",
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6):\n",
        "\n",
        "        self.has_continuous_action_space = has_continuous_action_space\n",
        "\n",
        "        if has_continuous_action_space:\n",
        "            self.action_std = action_std_init\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "        \n",
        "        self.buffer = RolloutBuffer()\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
        "        self.optimizer = torch.optim.Adam([\n",
        "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
        "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
        "                    ])\n",
        "\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "        \n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "\n",
        "    def set_action_std(self, new_action_std):\n",
        "        \n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_std = new_action_std\n",
        "            self.policy.set_action_std(new_action_std)\n",
        "            self.policy_old.set_action_std(new_action_std)\n",
        "        \n",
        "        else:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"WARNING : Calling PPO::set_action_std() on discrete action space policy\")\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
        "        print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_std = self.action_std - action_std_decay_rate\n",
        "            self.action_std = round(self.action_std, 4)\n",
        "            if (self.action_std <= min_action_std):\n",
        "                self.action_std = min_action_std\n",
        "                print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
        "            else:\n",
        "                print(\"setting actor output action_std to : \", self.action_std)\n",
        "            self.set_action_std(self.action_std)\n",
        "\n",
        "        else:\n",
        "            print(\"WARNING : Calling PPO::decay_action_std() on discrete action space policy\")\n",
        "\n",
        "        print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def select_action(self, state):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            with torch.no_grad():\n",
        "                state = torch.FloatTensor(state).to(device)\n",
        "                action, action_logprob = self.policy_old.act(state)\n",
        "\n",
        "            self.buffer.states.append(state)\n",
        "            self.buffer.actions.append(action)\n",
        "            self.buffer.logprobs.append(action_logprob)\n",
        "\n",
        "            return action.detach().cpu().numpy().flatten()\n",
        "\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state = torch.FloatTensor(state).to(device)\n",
        "                action, action_logprob = self.policy_old.act(state)\n",
        "            \n",
        "            self.buffer.states.append(state)\n",
        "            self.buffer.actions.append(action)\n",
        "            self.buffer.logprobs.append(action_logprob)\n",
        "\n",
        "            return action.item()\n",
        "\n",
        "\n",
        "    def update(self):\n",
        "\n",
        "        # Monte Carlo estimate of returns\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "            \n",
        "        # Normalizing the rewards\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
        "\n",
        "        # convert list to tensor\n",
        "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
        "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
        "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
        "\n",
        "        \n",
        "        # Optimize policy for K epochs\n",
        "        for _ in range(self.K_epochs):\n",
        "\n",
        "            # Evaluating old actions and values\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "            # match state_values tensor dimensions with rewards tensor\n",
        "            state_values = torch.squeeze(state_values)\n",
        "            \n",
        "            # Finding the ratio (pi_theta / pi_theta__old)\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "            # Finding Surrogate Loss\n",
        "            advantages = rewards - state_values.detach()   \n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
        "\n",
        "            # final loss of clipped objective PPO\n",
        "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
        "            \n",
        "            # take gradient step\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "            \n",
        "        # Copy new weights into old policy\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        # clear buffer\n",
        "        self.buffer.clear()\n",
        "    \n",
        "    \n",
        "    def save(self, checkpoint_path):\n",
        "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
        "   \n",
        "\n",
        "    def load(self, checkpoint_path):\n",
        "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
        "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        \n",
        "       \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr-ZjT_CGyEi"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - II**\n",
        "\n",
        "*   train PPO algorithm on environments\n",
        "*   save preTrained networks weights and log files\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY1-DzVCF8eh",
        "outputId": "a0383cf9-7294-452b-ddda-55fcc07f8a57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================================================\n",
            "training environment name : BipedalWalker-v2\n",
            "current logging run number for BipedalWalker-v2 :  0\n",
            "logging at : PPO_logs/BipedalWalker-v2//PPO_BipedalWalker-v2_log_0.csv\n",
            "save checkpoint path : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "--------------------------------------------------------------------------------------------\n",
            "max training timesteps :  4000000\n",
            "max timesteps per episode :  1500\n",
            "model saving frequency : 30000 timesteps\n",
            "log frequency : 3000 timesteps\n",
            "printing average reward over episodes in last : 6000 timesteps\n",
            "--------------------------------------------------------------------------------------------\n",
            "state space dimension :  24\n",
            "action space dimension :  4\n",
            "--------------------------------------------------------------------------------------------\n",
            "Initializing a continuous action space policy\n",
            "--------------------------------------------------------------------------------------------\n",
            "starting std of action distribution :  0.8\n",
            "decay rate of std of action distribution :  0.01\n",
            "minimum std of action distribution :  0.3\n",
            "decay frequency of std of action distribution : 30000 timesteps\n",
            "--------------------------------------------------------------------------------------------\n",
            "PPO update frequency : 6000 timesteps\n",
            "PPO K epochs :  40\n",
            "PPO epsilon clip :  0.2\n",
            "discount factor (gamma) :  0.99\n",
            "--------------------------------------------------------------------------------------------\n",
            "optimizer learning rate actor :  0.0003\n",
            "optimizer learning rate critic :  0.001\n",
            "============================================================================================\n",
            "Started training at (GMT) :  2022-02-28 13:43:19\n",
            "============================================================================================\n",
            "Episode : 6 \t\t Timestep : 6000 \t\t Average Reward : -97.8\n",
            "Episode : 24 \t\t Timestep : 12000 \t\t Average Reward : -103.3\n",
            "Episode : 35 \t\t Timestep : 18000 \t\t Average Reward : -104.55\n",
            "Episode : 45 \t\t Timestep : 24000 \t\t Average Reward : -96.8\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.79\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 52 \t\t Timestep : 30000 \t\t Average Reward : -93.81\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:00:42\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 60 \t\t Timestep : 36000 \t\t Average Reward : -96.49\n",
            "Episode : 64 \t\t Timestep : 42000 \t\t Average Reward : -81.2\n",
            "Episode : 70 \t\t Timestep : 48000 \t\t Average Reward : -92.17\n",
            "Episode : 77 \t\t Timestep : 54000 \t\t Average Reward : -93.31\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.78\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 86 \t\t Timestep : 60000 \t\t Average Reward : -96.89\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:01:16\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 91 \t\t Timestep : 66000 \t\t Average Reward : -82.42\n",
            "Episode : 95 \t\t Timestep : 72000 \t\t Average Reward : -77.64\n",
            "Episode : 100 \t\t Timestep : 78000 \t\t Average Reward : -86.25\n",
            "Episode : 106 \t\t Timestep : 84000 \t\t Average Reward : -86.42\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.77\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 111 \t\t Timestep : 90000 \t\t Average Reward : -85.92\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:01:52\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 118 \t\t Timestep : 96000 \t\t Average Reward : -89.8\n",
            "Episode : 126 \t\t Timestep : 102000 \t\t Average Reward : -92.27\n",
            "Episode : 130 \t\t Timestep : 108000 \t\t Average Reward : -72.57\n",
            "Episode : 135 \t\t Timestep : 114000 \t\t Average Reward : -81.2\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.76\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 141 \t\t Timestep : 120000 \t\t Average Reward : -85.19\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:02:26\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 145 \t\t Timestep : 126000 \t\t Average Reward : -74.46\n",
            "Episode : 149 \t\t Timestep : 132000 \t\t Average Reward : -75.58\n",
            "Episode : 154 \t\t Timestep : 138000 \t\t Average Reward : -83.27\n",
            "Episode : 159 \t\t Timestep : 144000 \t\t Average Reward : -83.34\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.75\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 163 \t\t Timestep : 150000 \t\t Average Reward : -75.77\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:03:00\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 168 \t\t Timestep : 156000 \t\t Average Reward : -79.95\n",
            "Episode : 174 \t\t Timestep : 162000 \t\t Average Reward : -79.87\n",
            "Episode : 179 \t\t Timestep : 168000 \t\t Average Reward : -80.48\n",
            "Episode : 183 \t\t Timestep : 174000 \t\t Average Reward : -75.16\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.74\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 187 \t\t Timestep : 180000 \t\t Average Reward : -83.13\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:03:34\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 191 \t\t Timestep : 186000 \t\t Average Reward : -67.93\n",
            "Episode : 195 \t\t Timestep : 192000 \t\t Average Reward : -69.46\n",
            "Episode : 199 \t\t Timestep : 198000 \t\t Average Reward : -65.38\n",
            "Episode : 204 \t\t Timestep : 204000 \t\t Average Reward : -74.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.73\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 208 \t\t Timestep : 210000 \t\t Average Reward : -65.95\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:04:08\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 212 \t\t Timestep : 216000 \t\t Average Reward : -62.47\n",
            "Episode : 216 \t\t Timestep : 222000 \t\t Average Reward : -60.56\n",
            "Episode : 220 \t\t Timestep : 228000 \t\t Average Reward : -63.52\n",
            "Episode : 225 \t\t Timestep : 234000 \t\t Average Reward : -69.85\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.72\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 229 \t\t Timestep : 240000 \t\t Average Reward : -56.6\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:04:42\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 233 \t\t Timestep : 246000 \t\t Average Reward : -56.0\n",
            "Episode : 238 \t\t Timestep : 252000 \t\t Average Reward : -65.01\n",
            "Episode : 243 \t\t Timestep : 258000 \t\t Average Reward : -106.19\n",
            "Episode : 248 \t\t Timestep : 264000 \t\t Average Reward : -68.43\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.71\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 252 \t\t Timestep : 270000 \t\t Average Reward : -49.29\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:05:15\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 256 \t\t Timestep : 276000 \t\t Average Reward : -72.21\n",
            "Episode : 260 \t\t Timestep : 282000 \t\t Average Reward : -47.16\n",
            "Episode : 265 \t\t Timestep : 288000 \t\t Average Reward : -62.34\n",
            "Episode : 269 \t\t Timestep : 294000 \t\t Average Reward : -47.09\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.7\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 273 \t\t Timestep : 300000 \t\t Average Reward : -47.56\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:05:49\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 277 \t\t Timestep : 306000 \t\t Average Reward : -32.8\n",
            "Episode : 281 \t\t Timestep : 312000 \t\t Average Reward : -36.7\n",
            "Episode : 285 \t\t Timestep : 318000 \t\t Average Reward : -57.51\n",
            "Episode : 289 \t\t Timestep : 324000 \t\t Average Reward : -40.85\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.69\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 294 \t\t Timestep : 330000 \t\t Average Reward : -46.94\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:06:22\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 298 \t\t Timestep : 336000 \t\t Average Reward : -54.41\n",
            "Episode : 302 \t\t Timestep : 342000 \t\t Average Reward : -32.37\n",
            "Episode : 306 \t\t Timestep : 348000 \t\t Average Reward : -26.56\n",
            "Episode : 310 \t\t Timestep : 354000 \t\t Average Reward : -30.91\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.68\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 315 \t\t Timestep : 360000 \t\t Average Reward : -35.67\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:06:56\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 319 \t\t Timestep : 366000 \t\t Average Reward : -27.1\n",
            "Episode : 323 \t\t Timestep : 372000 \t\t Average Reward : -18.41\n",
            "Episode : 327 \t\t Timestep : 378000 \t\t Average Reward : -10.66\n",
            "Episode : 331 \t\t Timestep : 384000 \t\t Average Reward : -1.88\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.67\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 335 \t\t Timestep : 390000 \t\t Average Reward : 1.49\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:07:29\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 340 \t\t Timestep : 396000 \t\t Average Reward : -18.66\n",
            "Episode : 344 \t\t Timestep : 402000 \t\t Average Reward : -4.19\n",
            "Episode : 348 \t\t Timestep : 408000 \t\t Average Reward : 3.81\n",
            "Episode : 352 \t\t Timestep : 414000 \t\t Average Reward : -0.51\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.66\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 356 \t\t Timestep : 420000 \t\t Average Reward : 3.87\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:08:04\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 360 \t\t Timestep : 426000 \t\t Average Reward : 17.49\n",
            "Episode : 364 \t\t Timestep : 432000 \t\t Average Reward : 15.42\n",
            "Episode : 370 \t\t Timestep : 438000 \t\t Average Reward : -44.65\n",
            "Episode : 374 \t\t Timestep : 444000 \t\t Average Reward : 16.01\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.65\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 378 \t\t Timestep : 450000 \t\t Average Reward : 26.87\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:08:38\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 382 \t\t Timestep : 456000 \t\t Average Reward : 24.87\n",
            "Episode : 386 \t\t Timestep : 462000 \t\t Average Reward : 23.88\n",
            "Episode : 390 \t\t Timestep : 468000 \t\t Average Reward : 30.52\n",
            "Episode : 394 \t\t Timestep : 474000 \t\t Average Reward : 26.19\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.64\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 398 \t\t Timestep : 480000 \t\t Average Reward : 29.73\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:09:12\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 402 \t\t Timestep : 486000 \t\t Average Reward : 36.65\n",
            "Episode : 406 \t\t Timestep : 492000 \t\t Average Reward : 43.28\n",
            "Episode : 412 \t\t Timestep : 498000 \t\t Average Reward : -2.77\n",
            "Episode : 416 \t\t Timestep : 504000 \t\t Average Reward : 12.42\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.63\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 420 \t\t Timestep : 510000 \t\t Average Reward : 45.24\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:09:46\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 424 \t\t Timestep : 516000 \t\t Average Reward : 55.56\n",
            "Episode : 428 \t\t Timestep : 522000 \t\t Average Reward : 15.37\n",
            "Episode : 433 \t\t Timestep : 528000 \t\t Average Reward : 14.97\n",
            "Episode : 437 \t\t Timestep : 534000 \t\t Average Reward : 51.98\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.62\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 441 \t\t Timestep : 540000 \t\t Average Reward : 58.06\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:10:21\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 445 \t\t Timestep : 546000 \t\t Average Reward : 49.5\n",
            "Episode : 450 \t\t Timestep : 552000 \t\t Average Reward : 27.89\n",
            "Episode : 454 \t\t Timestep : 558000 \t\t Average Reward : 67.23\n",
            "Episode : 458 \t\t Timestep : 564000 \t\t Average Reward : 58.39\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.61\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 462 \t\t Timestep : 570000 \t\t Average Reward : 57.86\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:10:55\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 466 \t\t Timestep : 576000 \t\t Average Reward : 12.67\n",
            "Episode : 471 \t\t Timestep : 582000 \t\t Average Reward : 31.0\n",
            "Episode : 476 \t\t Timestep : 588000 \t\t Average Reward : 24.39\n",
            "Episode : 481 \t\t Timestep : 594000 \t\t Average Reward : 20.79\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.6\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 485 \t\t Timestep : 600000 \t\t Average Reward : 65.82\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:11:29\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 489 \t\t Timestep : 606000 \t\t Average Reward : 74.23\n",
            "Episode : 493 \t\t Timestep : 612000 \t\t Average Reward : 71.03\n",
            "Episode : 498 \t\t Timestep : 618000 \t\t Average Reward : 22.39\n",
            "Episode : 502 \t\t Timestep : 624000 \t\t Average Reward : 32.8\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.59\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 507 \t\t Timestep : 630000 \t\t Average Reward : 41.01\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:12:04\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 512 \t\t Timestep : 636000 \t\t Average Reward : 65.51\n",
            "Episode : 516 \t\t Timestep : 642000 \t\t Average Reward : 84.67\n",
            "Episode : 521 \t\t Timestep : 648000 \t\t Average Reward : 50.5\n",
            "Episode : 525 \t\t Timestep : 654000 \t\t Average Reward : 46.27\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.58\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 530 \t\t Timestep : 660000 \t\t Average Reward : 50.45\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:12:38\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 535 \t\t Timestep : 666000 \t\t Average Reward : 44.45\n",
            "Episode : 540 \t\t Timestep : 672000 \t\t Average Reward : 27.65\n",
            "Episode : 544 \t\t Timestep : 678000 \t\t Average Reward : 86.6\n",
            "Episode : 548 \t\t Timestep : 684000 \t\t Average Reward : 101.18\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.57\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 554 \t\t Timestep : 690000 \t\t Average Reward : 23.48\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:13:12\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 559 \t\t Timestep : 696000 \t\t Average Reward : 10.89\n",
            "Episode : 563 \t\t Timestep : 702000 \t\t Average Reward : 70.46\n",
            "Episode : 569 \t\t Timestep : 708000 \t\t Average Reward : 4.17\n",
            "Episode : 573 \t\t Timestep : 714000 \t\t Average Reward : 87.72\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.56\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 577 \t\t Timestep : 720000 \t\t Average Reward : 72.99\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:13:46\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 582 \t\t Timestep : 726000 \t\t Average Reward : 68.93\n",
            "Episode : 586 \t\t Timestep : 732000 \t\t Average Reward : 99.68\n",
            "Episode : 592 \t\t Timestep : 738000 \t\t Average Reward : 45.33\n",
            "Episode : 597 \t\t Timestep : 744000 \t\t Average Reward : 48.36\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.55\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 601 \t\t Timestep : 750000 \t\t Average Reward : 114.52\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:14:20\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 606 \t\t Timestep : 756000 \t\t Average Reward : 47.46\n",
            "Episode : 611 \t\t Timestep : 762000 \t\t Average Reward : 17.66\n",
            "Episode : 616 \t\t Timestep : 768000 \t\t Average Reward : 74.75\n",
            "Episode : 620 \t\t Timestep : 774000 \t\t Average Reward : 115.06\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.54\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 624 \t\t Timestep : 780000 \t\t Average Reward : 115.71\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:14:54\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 628 \t\t Timestep : 786000 \t\t Average Reward : 61.67\n",
            "Episode : 632 \t\t Timestep : 792000 \t\t Average Reward : 113.76\n",
            "Episode : 636 \t\t Timestep : 798000 \t\t Average Reward : 119.22\n",
            "Episode : 640 \t\t Timestep : 804000 \t\t Average Reward : 129.46\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.53\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 645 \t\t Timestep : 810000 \t\t Average Reward : 94.99\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:15:28\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 649 \t\t Timestep : 816000 \t\t Average Reward : 138.82\n",
            "Episode : 653 \t\t Timestep : 822000 \t\t Average Reward : 120.83\n",
            "Episode : 657 \t\t Timestep : 828000 \t\t Average Reward : 136.58\n",
            "Episode : 662 \t\t Timestep : 834000 \t\t Average Reward : 87.13\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.52\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 666 \t\t Timestep : 840000 \t\t Average Reward : 137.95\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:16:03\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 670 \t\t Timestep : 846000 \t\t Average Reward : 141.58\n",
            "Episode : 674 \t\t Timestep : 852000 \t\t Average Reward : 107.92\n",
            "Episode : 678 \t\t Timestep : 858000 \t\t Average Reward : 140.06\n",
            "Episode : 684 \t\t Timestep : 864000 \t\t Average Reward : 56.85\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.51\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 688 \t\t Timestep : 870000 \t\t Average Reward : 116.18\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:16:37\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 692 \t\t Timestep : 876000 \t\t Average Reward : 153.69\n",
            "Episode : 696 \t\t Timestep : 882000 \t\t Average Reward : 152.44\n",
            "Episode : 701 \t\t Timestep : 888000 \t\t Average Reward : 93.21\n",
            "Episode : 705 \t\t Timestep : 894000 \t\t Average Reward : 82.83\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.5\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 709 \t\t Timestep : 900000 \t\t Average Reward : 149.65\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:17:11\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 714 \t\t Timestep : 906000 \t\t Average Reward : 121.31\n",
            "Episode : 721 \t\t Timestep : 912000 \t\t Average Reward : 21.03\n",
            "Episode : 725 \t\t Timestep : 918000 \t\t Average Reward : 129.09\n",
            "Episode : 729 \t\t Timestep : 924000 \t\t Average Reward : 151.92\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.49\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 733 \t\t Timestep : 930000 \t\t Average Reward : 146.99\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:17:45\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 737 \t\t Timestep : 936000 \t\t Average Reward : 152.58\n",
            "Episode : 743 \t\t Timestep : 942000 \t\t Average Reward : 70.15\n",
            "Episode : 747 \t\t Timestep : 948000 \t\t Average Reward : 158.81\n",
            "Episode : 751 \t\t Timestep : 954000 \t\t Average Reward : 98.21\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.48\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 755 \t\t Timestep : 960000 \t\t Average Reward : 156.28\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:18:19\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 762 \t\t Timestep : 966000 \t\t Average Reward : 63.77\n",
            "Episode : 766 \t\t Timestep : 972000 \t\t Average Reward : 128.45\n",
            "Episode : 771 \t\t Timestep : 978000 \t\t Average Reward : 121.5\n",
            "Episode : 775 \t\t Timestep : 984000 \t\t Average Reward : 160.65\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.47\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 779 \t\t Timestep : 990000 \t\t Average Reward : 153.41\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:18:53\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 783 \t\t Timestep : 996000 \t\t Average Reward : 131.61\n",
            "Episode : 787 \t\t Timestep : 1002000 \t\t Average Reward : 169.39\n",
            "Episode : 793 \t\t Timestep : 1008000 \t\t Average Reward : 50.85\n",
            "Episode : 798 \t\t Timestep : 1014000 \t\t Average Reward : 116.99\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.46\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 802 \t\t Timestep : 1020000 \t\t Average Reward : 167.89\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:19:27\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 806 \t\t Timestep : 1026000 \t\t Average Reward : 127.87\n",
            "Episode : 812 \t\t Timestep : 1032000 \t\t Average Reward : 18.68\n",
            "Episode : 817 \t\t Timestep : 1038000 \t\t Average Reward : 60.41\n",
            "Episode : 822 \t\t Timestep : 1044000 \t\t Average Reward : 139.64\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.45\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 826 \t\t Timestep : 1050000 \t\t Average Reward : 97.73\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:20:01\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 830 \t\t Timestep : 1056000 \t\t Average Reward : 170.87\n",
            "Episode : 835 \t\t Timestep : 1062000 \t\t Average Reward : 139.66\n",
            "Episode : 839 \t\t Timestep : 1068000 \t\t Average Reward : 168.37\n",
            "Episode : 843 \t\t Timestep : 1074000 \t\t Average Reward : 177.14\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.44\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 847 \t\t Timestep : 1080000 \t\t Average Reward : 109.69\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:20:35\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 852 \t\t Timestep : 1086000 \t\t Average Reward : 123.41\n",
            "Episode : 856 \t\t Timestep : 1092000 \t\t Average Reward : 175.35\n",
            "Episode : 861 \t\t Timestep : 1098000 \t\t Average Reward : 135.56\n",
            "Episode : 865 \t\t Timestep : 1104000 \t\t Average Reward : 171.87\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.43\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 869 \t\t Timestep : 1110000 \t\t Average Reward : 172.46\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:21:09\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 874 \t\t Timestep : 1116000 \t\t Average Reward : 72.21\n",
            "Episode : 878 \t\t Timestep : 1122000 \t\t Average Reward : 179.2\n",
            "Episode : 883 \t\t Timestep : 1128000 \t\t Average Reward : 138.28\n",
            "Episode : 887 \t\t Timestep : 1134000 \t\t Average Reward : 117.81\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.42\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 892 \t\t Timestep : 1140000 \t\t Average Reward : 121.79\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:21:43\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 897 \t\t Timestep : 1146000 \t\t Average Reward : 140.89\n",
            "Episode : 901 \t\t Timestep : 1152000 \t\t Average Reward : 131.88\n",
            "Episode : 906 \t\t Timestep : 1158000 \t\t Average Reward : 137.02\n",
            "Episode : 911 \t\t Timestep : 1164000 \t\t Average Reward : 76.39\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.41\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 915 \t\t Timestep : 1170000 \t\t Average Reward : 181.67\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:22:18\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 919 \t\t Timestep : 1176000 \t\t Average Reward : 175.94\n",
            "Episode : 925 \t\t Timestep : 1182000 \t\t Average Reward : 92.31\n",
            "Episode : 929 \t\t Timestep : 1188000 \t\t Average Reward : 180.04\n",
            "Episode : 933 \t\t Timestep : 1194000 \t\t Average Reward : 143.74\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.4\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 938 \t\t Timestep : 1200000 \t\t Average Reward : 142.19\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:22:52\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 943 \t\t Timestep : 1206000 \t\t Average Reward : 94.99\n",
            "Episode : 947 \t\t Timestep : 1212000 \t\t Average Reward : 180.44\n",
            "Episode : 951 \t\t Timestep : 1218000 \t\t Average Reward : 183.31\n",
            "Episode : 955 \t\t Timestep : 1224000 \t\t Average Reward : 177.61\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.39\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 959 \t\t Timestep : 1230000 \t\t Average Reward : 184.03\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:23:26\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 963 \t\t Timestep : 1236000 \t\t Average Reward : 177.6\n",
            "Episode : 968 \t\t Timestep : 1242000 \t\t Average Reward : 128.84\n",
            "Episode : 972 \t\t Timestep : 1248000 \t\t Average Reward : 193.36\n",
            "Episode : 976 \t\t Timestep : 1254000 \t\t Average Reward : 181.73\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.38\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 980 \t\t Timestep : 1260000 \t\t Average Reward : 177.6\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:24:00\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 984 \t\t Timestep : 1266000 \t\t Average Reward : 186.83\n",
            "Episode : 988 \t\t Timestep : 1272000 \t\t Average Reward : 183.31\n",
            "Episode : 992 \t\t Timestep : 1278000 \t\t Average Reward : 186.51\n",
            "Episode : 996 \t\t Timestep : 1284000 \t\t Average Reward : 130.68\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.37\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1000 \t\t Timestep : 1290000 \t\t Average Reward : 183.55\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:24:34\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1004 \t\t Timestep : 1296000 \t\t Average Reward : 188.3\n",
            "Episode : 1009 \t\t Timestep : 1302000 \t\t Average Reward : 139.2\n",
            "Episode : 1013 \t\t Timestep : 1308000 \t\t Average Reward : 189.09\n",
            "Episode : 1017 \t\t Timestep : 1314000 \t\t Average Reward : 185.56\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.36\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1021 \t\t Timestep : 1320000 \t\t Average Reward : 193.81\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:25:08\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1026 \t\t Timestep : 1326000 \t\t Average Reward : 94.69\n",
            "Episode : 1030 \t\t Timestep : 1332000 \t\t Average Reward : 196.94\n",
            "Episode : 1035 \t\t Timestep : 1338000 \t\t Average Reward : 154.27\n",
            "Episode : 1039 \t\t Timestep : 1344000 \t\t Average Reward : 197.6\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.35\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1043 \t\t Timestep : 1350000 \t\t Average Reward : 189.24\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:25:42\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1047 \t\t Timestep : 1356000 \t\t Average Reward : 197.78\n",
            "Episode : 1053 \t\t Timestep : 1362000 \t\t Average Reward : 62.9\n",
            "Episode : 1059 \t\t Timestep : 1368000 \t\t Average Reward : 75.26\n",
            "Episode : 1063 \t\t Timestep : 1374000 \t\t Average Reward : 191.17\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.34\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1067 \t\t Timestep : 1380000 \t\t Average Reward : 169.35\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:26:16\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1071 \t\t Timestep : 1386000 \t\t Average Reward : 199.44\n",
            "Episode : 1075 \t\t Timestep : 1392000 \t\t Average Reward : 202.17\n",
            "Episode : 1080 \t\t Timestep : 1398000 \t\t Average Reward : 142.8\n",
            "Episode : 1085 \t\t Timestep : 1404000 \t\t Average Reward : 131.24\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.33\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1089 \t\t Timestep : 1410000 \t\t Average Reward : 205.5\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:26:50\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1093 \t\t Timestep : 1416000 \t\t Average Reward : 206.63\n",
            "Episode : 1098 \t\t Timestep : 1422000 \t\t Average Reward : 101.33\n",
            "Episode : 1102 \t\t Timestep : 1428000 \t\t Average Reward : 210.01\n",
            "Episode : 1107 \t\t Timestep : 1434000 \t\t Average Reward : 89.32\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.32\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1112 \t\t Timestep : 1440000 \t\t Average Reward : 153.51\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:27:24\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1116 \t\t Timestep : 1446000 \t\t Average Reward : 178.11\n",
            "Episode : 1121 \t\t Timestep : 1452000 \t\t Average Reward : 174.13\n",
            "Episode : 1126 \t\t Timestep : 1458000 \t\t Average Reward : 82.1\n",
            "Episode : 1130 \t\t Timestep : 1464000 \t\t Average Reward : 201.13\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.31\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1134 \t\t Timestep : 1470000 \t\t Average Reward : 216.94\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:27:58\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1138 \t\t Timestep : 1476000 \t\t Average Reward : 211.01\n",
            "Episode : 1144 \t\t Timestep : 1482000 \t\t Average Reward : 41.18\n",
            "Episode : 1150 \t\t Timestep : 1488000 \t\t Average Reward : 94.99\n",
            "Episode : 1154 \t\t Timestep : 1494000 \t\t Average Reward : 153.6\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1158 \t\t Timestep : 1500000 \t\t Average Reward : 218.88\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:28:32\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1163 \t\t Timestep : 1506000 \t\t Average Reward : 189.86\n",
            "Episode : 1167 \t\t Timestep : 1512000 \t\t Average Reward : 221.62\n",
            "Episode : 1171 \t\t Timestep : 1518000 \t\t Average Reward : 112.41\n",
            "Episode : 1175 \t\t Timestep : 1524000 \t\t Average Reward : 220.09\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1180 \t\t Timestep : 1530000 \t\t Average Reward : 189.69\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:29:06\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1184 \t\t Timestep : 1536000 \t\t Average Reward : 198.4\n",
            "Episode : 1188 \t\t Timestep : 1542000 \t\t Average Reward : 180.79\n",
            "Episode : 1192 \t\t Timestep : 1548000 \t\t Average Reward : 215.53\n",
            "Episode : 1197 \t\t Timestep : 1554000 \t\t Average Reward : 100.2\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1201 \t\t Timestep : 1560000 \t\t Average Reward : 221.54\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:29:40\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1206 \t\t Timestep : 1566000 \t\t Average Reward : 162.94\n",
            "Episode : 1210 \t\t Timestep : 1572000 \t\t Average Reward : 224.32\n",
            "Episode : 1214 \t\t Timestep : 1578000 \t\t Average Reward : 216.55\n",
            "Episode : 1218 \t\t Timestep : 1584000 \t\t Average Reward : 225.02\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1222 \t\t Timestep : 1590000 \t\t Average Reward : 218.35\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:30:14\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1227 \t\t Timestep : 1596000 \t\t Average Reward : 152.51\n",
            "Episode : 1231 \t\t Timestep : 1602000 \t\t Average Reward : 140.68\n",
            "Episode : 1235 \t\t Timestep : 1608000 \t\t Average Reward : 224.41\n",
            "Episode : 1240 \t\t Timestep : 1614000 \t\t Average Reward : 194.52\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1244 \t\t Timestep : 1620000 \t\t Average Reward : 146.45\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:30:48\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1251 \t\t Timestep : 1626000 \t\t Average Reward : 98.91\n",
            "Episode : 1257 \t\t Timestep : 1632000 \t\t Average Reward : 92.02\n",
            "Episode : 1261 \t\t Timestep : 1638000 \t\t Average Reward : 175.84\n",
            "Episode : 1265 \t\t Timestep : 1644000 \t\t Average Reward : 198.32\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1270 \t\t Timestep : 1650000 \t\t Average Reward : 131.94\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:31:22\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1274 \t\t Timestep : 1656000 \t\t Average Reward : 222.3\n",
            "Episode : 1278 \t\t Timestep : 1662000 \t\t Average Reward : 225.68\n",
            "Episode : 1283 \t\t Timestep : 1668000 \t\t Average Reward : 127.41\n",
            "Episode : 1287 \t\t Timestep : 1674000 \t\t Average Reward : 157.48\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1292 \t\t Timestep : 1680000 \t\t Average Reward : 177.72\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:31:56\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1296 \t\t Timestep : 1686000 \t\t Average Reward : 229.74\n",
            "Episode : 1300 \t\t Timestep : 1692000 \t\t Average Reward : 232.1\n",
            "Episode : 1306 \t\t Timestep : 1698000 \t\t Average Reward : 86.52\n",
            "Episode : 1311 \t\t Timestep : 1704000 \t\t Average Reward : 93.42\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1315 \t\t Timestep : 1710000 \t\t Average Reward : 195.02\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:32:30\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1320 \t\t Timestep : 1716000 \t\t Average Reward : 202.3\n",
            "Episode : 1324 \t\t Timestep : 1722000 \t\t Average Reward : 146.32\n",
            "Episode : 1328 \t\t Timestep : 1728000 \t\t Average Reward : 202.31\n",
            "Episode : 1333 \t\t Timestep : 1734000 \t\t Average Reward : 205.49\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1337 \t\t Timestep : 1740000 \t\t Average Reward : 231.28\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:33:04\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1341 \t\t Timestep : 1746000 \t\t Average Reward : 199.06\n",
            "Episode : 1345 \t\t Timestep : 1752000 \t\t Average Reward : 166.94\n",
            "Episode : 1349 \t\t Timestep : 1758000 \t\t Average Reward : 234.52\n",
            "Episode : 1353 \t\t Timestep : 1764000 \t\t Average Reward : 225.89\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1359 \t\t Timestep : 1770000 \t\t Average Reward : 139.67\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:33:38\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1363 \t\t Timestep : 1776000 \t\t Average Reward : 232.18\n",
            "Episode : 1367 \t\t Timestep : 1782000 \t\t Average Reward : 230.45\n",
            "Episode : 1372 \t\t Timestep : 1788000 \t\t Average Reward : 121.65\n",
            "Episode : 1376 \t\t Timestep : 1794000 \t\t Average Reward : 178.37\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1380 \t\t Timestep : 1800000 \t\t Average Reward : 229.23\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:34:12\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1384 \t\t Timestep : 1806000 \t\t Average Reward : 235.74\n",
            "Episode : 1388 \t\t Timestep : 1812000 \t\t Average Reward : 182.82\n",
            "Episode : 1392 \t\t Timestep : 1818000 \t\t Average Reward : 237.71\n",
            "Episode : 1396 \t\t Timestep : 1824000 \t\t Average Reward : 234.89\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1400 \t\t Timestep : 1830000 \t\t Average Reward : 229.88\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:34:46\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1404 \t\t Timestep : 1836000 \t\t Average Reward : 235.7\n",
            "Episode : 1409 \t\t Timestep : 1842000 \t\t Average Reward : 176.83\n",
            "Episode : 1415 \t\t Timestep : 1848000 \t\t Average Reward : 126.91\n",
            "Episode : 1419 \t\t Timestep : 1854000 \t\t Average Reward : 212.27\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1424 \t\t Timestep : 1860000 \t\t Average Reward : 144.26\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:35:20\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1428 \t\t Timestep : 1866000 \t\t Average Reward : 240.17\n",
            "Episode : 1432 \t\t Timestep : 1872000 \t\t Average Reward : 236.89\n",
            "Episode : 1436 \t\t Timestep : 1878000 \t\t Average Reward : 243.34\n",
            "Episode : 1440 \t\t Timestep : 1884000 \t\t Average Reward : 201.51\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1444 \t\t Timestep : 1890000 \t\t Average Reward : 242.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:35:54\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1448 \t\t Timestep : 1896000 \t\t Average Reward : 239.58\n",
            "Episode : 1452 \t\t Timestep : 1902000 \t\t Average Reward : 240.94\n",
            "Episode : 1457 \t\t Timestep : 1908000 \t\t Average Reward : 169.1\n",
            "Episode : 1461 \t\t Timestep : 1914000 \t\t Average Reward : 244.45\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1465 \t\t Timestep : 1920000 \t\t Average Reward : 249.29\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:36:28\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1471 \t\t Timestep : 1926000 \t\t Average Reward : 84.35\n",
            "Episode : 1475 \t\t Timestep : 1932000 \t\t Average Reward : 203.8\n",
            "Episode : 1480 \t\t Timestep : 1938000 \t\t Average Reward : 184.45\n",
            "Episode : 1484 \t\t Timestep : 1944000 \t\t Average Reward : 250.12\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1488 \t\t Timestep : 1950000 \t\t Average Reward : 243.73\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:37:02\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1493 \t\t Timestep : 1956000 \t\t Average Reward : 177.09\n",
            "Episode : 1497 \t\t Timestep : 1962000 \t\t Average Reward : 244.66\n",
            "Episode : 1502 \t\t Timestep : 1968000 \t\t Average Reward : 139.34\n",
            "Episode : 1507 \t\t Timestep : 1974000 \t\t Average Reward : 190.53\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1511 \t\t Timestep : 1980000 \t\t Average Reward : 177.76\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:37:36\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1516 \t\t Timestep : 1986000 \t\t Average Reward : 188.54\n",
            "Episode : 1521 \t\t Timestep : 1992000 \t\t Average Reward : 165.11\n",
            "Episode : 1525 \t\t Timestep : 1998000 \t\t Average Reward : 241.8\n",
            "Episode : 1529 \t\t Timestep : 2004000 \t\t Average Reward : 240.74\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1534 \t\t Timestep : 2010000 \t\t Average Reward : 160.76\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:38:10\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1538 \t\t Timestep : 2016000 \t\t Average Reward : 183.0\n",
            "Episode : 1543 \t\t Timestep : 2022000 \t\t Average Reward : 180.28\n",
            "Episode : 1547 \t\t Timestep : 2028000 \t\t Average Reward : 244.62\n",
            "Episode : 1553 \t\t Timestep : 2034000 \t\t Average Reward : 83.93\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1557 \t\t Timestep : 2040000 \t\t Average Reward : 203.78\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:38:44\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1561 \t\t Timestep : 2046000 \t\t Average Reward : 248.28\n",
            "Episode : 1565 \t\t Timestep : 2052000 \t\t Average Reward : 250.34\n",
            "Episode : 1571 \t\t Timestep : 2058000 \t\t Average Reward : 127.98\n",
            "Episode : 1575 \t\t Timestep : 2064000 \t\t Average Reward : 248.76\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1579 \t\t Timestep : 2070000 \t\t Average Reward : 242.85\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:39:18\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1583 \t\t Timestep : 2076000 \t\t Average Reward : 212.15\n",
            "Episode : 1588 \t\t Timestep : 2082000 \t\t Average Reward : 187.12\n",
            "Episode : 1592 \t\t Timestep : 2088000 \t\t Average Reward : 251.99\n",
            "Episode : 1596 \t\t Timestep : 2094000 \t\t Average Reward : 240.84\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1601 \t\t Timestep : 2100000 \t\t Average Reward : 195.39\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:39:52\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1605 \t\t Timestep : 2106000 \t\t Average Reward : 184.4\n",
            "Episode : 1609 \t\t Timestep : 2112000 \t\t Average Reward : 246.7\n",
            "Episode : 1616 \t\t Timestep : 2118000 \t\t Average Reward : 105.89\n",
            "Episode : 1620 \t\t Timestep : 2124000 \t\t Average Reward : 198.26\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1624 \t\t Timestep : 2130000 \t\t Average Reward : 222.86\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:40:26\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1628 \t\t Timestep : 2136000 \t\t Average Reward : 239.5\n",
            "Episode : 1633 \t\t Timestep : 2142000 \t\t Average Reward : 202.35\n",
            "Episode : 1638 \t\t Timestep : 2148000 \t\t Average Reward : 118.7\n",
            "Episode : 1642 \t\t Timestep : 2154000 \t\t Average Reward : 250.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1646 \t\t Timestep : 2160000 \t\t Average Reward : 255.13\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:41:00\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1650 \t\t Timestep : 2166000 \t\t Average Reward : 210.47\n",
            "Episode : 1656 \t\t Timestep : 2172000 \t\t Average Reward : 126.72\n",
            "Episode : 1660 \t\t Timestep : 2178000 \t\t Average Reward : 247.41\n",
            "Episode : 1664 \t\t Timestep : 2184000 \t\t Average Reward : 213.51\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1668 \t\t Timestep : 2190000 \t\t Average Reward : 175.1\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:41:34\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1674 \t\t Timestep : 2196000 \t\t Average Reward : 145.03\n",
            "Episode : 1679 \t\t Timestep : 2202000 \t\t Average Reward : 184.79\n",
            "Episode : 1683 \t\t Timestep : 2208000 \t\t Average Reward : 242.93\n",
            "Episode : 1687 \t\t Timestep : 2214000 \t\t Average Reward : 256.42\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1691 \t\t Timestep : 2220000 \t\t Average Reward : 251.15\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:42:08\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1695 \t\t Timestep : 2226000 \t\t Average Reward : 196.28\n",
            "Episode : 1699 \t\t Timestep : 2232000 \t\t Average Reward : 247.99\n",
            "Episode : 1703 \t\t Timestep : 2238000 \t\t Average Reward : 213.29\n",
            "Episode : 1707 \t\t Timestep : 2244000 \t\t Average Reward : 251.04\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1712 \t\t Timestep : 2250000 \t\t Average Reward : 189.44\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:42:42\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1716 \t\t Timestep : 2256000 \t\t Average Reward : 238.4\n",
            "Episode : 1720 \t\t Timestep : 2262000 \t\t Average Reward : 247.27\n",
            "Episode : 1725 \t\t Timestep : 2268000 \t\t Average Reward : 107.06\n",
            "Episode : 1729 \t\t Timestep : 2274000 \t\t Average Reward : 228.82\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1733 \t\t Timestep : 2280000 \t\t Average Reward : 245.89\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:43:16\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1737 \t\t Timestep : 2286000 \t\t Average Reward : 245.77\n",
            "Episode : 1742 \t\t Timestep : 2292000 \t\t Average Reward : 175.02\n",
            "Episode : 1746 \t\t Timestep : 2298000 \t\t Average Reward : 251.84\n",
            "Episode : 1750 \t\t Timestep : 2304000 \t\t Average Reward : 249.09\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1754 \t\t Timestep : 2310000 \t\t Average Reward : 249.85\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:43:50\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1758 \t\t Timestep : 2316000 \t\t Average Reward : 219.38\n",
            "Episode : 1762 \t\t Timestep : 2322000 \t\t Average Reward : 237.53\n",
            "Episode : 1766 \t\t Timestep : 2328000 \t\t Average Reward : 248.32\n",
            "Episode : 1771 \t\t Timestep : 2334000 \t\t Average Reward : 200.62\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1775 \t\t Timestep : 2340000 \t\t Average Reward : 248.59\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:44:24\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1779 \t\t Timestep : 2346000 \t\t Average Reward : 184.24\n",
            "Episode : 1783 \t\t Timestep : 2352000 \t\t Average Reward : 238.62\n",
            "Episode : 1787 \t\t Timestep : 2358000 \t\t Average Reward : 213.22\n",
            "Episode : 1792 \t\t Timestep : 2364000 \t\t Average Reward : 189.28\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1796 \t\t Timestep : 2370000 \t\t Average Reward : 219.53\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:44:59\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1801 \t\t Timestep : 2376000 \t\t Average Reward : 189.22\n",
            "Episode : 1805 \t\t Timestep : 2382000 \t\t Average Reward : 242.0\n",
            "Episode : 1811 \t\t Timestep : 2388000 \t\t Average Reward : 123.95\n",
            "Episode : 1815 \t\t Timestep : 2394000 \t\t Average Reward : 209.74\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1820 \t\t Timestep : 2400000 \t\t Average Reward : 180.59\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:45:33\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1825 \t\t Timestep : 2406000 \t\t Average Reward : 178.31\n",
            "Episode : 1829 \t\t Timestep : 2412000 \t\t Average Reward : 247.39\n",
            "Episode : 1833 \t\t Timestep : 2418000 \t\t Average Reward : 191.0\n",
            "Episode : 1838 \t\t Timestep : 2424000 \t\t Average Reward : 196.23\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1842 \t\t Timestep : 2430000 \t\t Average Reward : 210.37\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:46:07\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1846 \t\t Timestep : 2436000 \t\t Average Reward : 172.61\n",
            "Episode : 1850 \t\t Timestep : 2442000 \t\t Average Reward : 249.15\n",
            "Episode : 1855 \t\t Timestep : 2448000 \t\t Average Reward : 185.66\n",
            "Episode : 1860 \t\t Timestep : 2454000 \t\t Average Reward : 208.27\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1864 \t\t Timestep : 2460000 \t\t Average Reward : 247.78\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:46:40\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1869 \t\t Timestep : 2466000 \t\t Average Reward : 145.37\n",
            "Episode : 1873 \t\t Timestep : 2472000 \t\t Average Reward : 206.29\n",
            "Episode : 1877 \t\t Timestep : 2478000 \t\t Average Reward : 246.8\n",
            "Episode : 1881 \t\t Timestep : 2484000 \t\t Average Reward : 251.26\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1885 \t\t Timestep : 2490000 \t\t Average Reward : 249.88\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:47:14\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1889 \t\t Timestep : 2496000 \t\t Average Reward : 245.57\n",
            "Episode : 1895 \t\t Timestep : 2502000 \t\t Average Reward : 119.32\n",
            "Episode : 1900 \t\t Timestep : 2508000 \t\t Average Reward : 133.91\n",
            "Episode : 1905 \t\t Timestep : 2514000 \t\t Average Reward : 169.47\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1909 \t\t Timestep : 2520000 \t\t Average Reward : 195.89\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:47:48\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1913 \t\t Timestep : 2526000 \t\t Average Reward : 246.38\n",
            "Episode : 1917 \t\t Timestep : 2532000 \t\t Average Reward : 210.27\n",
            "Episode : 1921 \t\t Timestep : 2538000 \t\t Average Reward : 248.57\n",
            "Episode : 1925 \t\t Timestep : 2544000 \t\t Average Reward : 242.14\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1929 \t\t Timestep : 2550000 \t\t Average Reward : 243.1\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:48:22\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1933 \t\t Timestep : 2556000 \t\t Average Reward : 244.02\n",
            "Episode : 1937 \t\t Timestep : 2562000 \t\t Average Reward : 243.01\n",
            "Episode : 1942 \t\t Timestep : 2568000 \t\t Average Reward : 190.05\n",
            "Episode : 1946 \t\t Timestep : 2574000 \t\t Average Reward : 200.14\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1950 \t\t Timestep : 2580000 \t\t Average Reward : 242.7\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:48:57\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1955 \t\t Timestep : 2586000 \t\t Average Reward : 177.1\n",
            "Episode : 1959 \t\t Timestep : 2592000 \t\t Average Reward : 245.99\n",
            "Episode : 1963 \t\t Timestep : 2598000 \t\t Average Reward : 183.24\n",
            "Episode : 1968 \t\t Timestep : 2604000 \t\t Average Reward : 196.29\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1972 \t\t Timestep : 2610000 \t\t Average Reward : 253.5\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:49:31\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1976 \t\t Timestep : 2616000 \t\t Average Reward : 241.95\n",
            "Episode : 1981 \t\t Timestep : 2622000 \t\t Average Reward : 143.96\n",
            "Episode : 1985 \t\t Timestep : 2628000 \t\t Average Reward : 245.77\n",
            "Episode : 1989 \t\t Timestep : 2634000 \t\t Average Reward : 247.8\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1993 \t\t Timestep : 2640000 \t\t Average Reward : 246.67\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:50:05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1998 \t\t Timestep : 2646000 \t\t Average Reward : 135.51\n",
            "Episode : 2003 \t\t Timestep : 2652000 \t\t Average Reward : 180.7\n",
            "Episode : 2007 \t\t Timestep : 2658000 \t\t Average Reward : 241.63\n",
            "Episode : 2011 \t\t Timestep : 2664000 \t\t Average Reward : 250.12\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2016 \t\t Timestep : 2670000 \t\t Average Reward : 195.41\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:50:39\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2020 \t\t Timestep : 2676000 \t\t Average Reward : 249.43\n",
            "Episode : 2024 \t\t Timestep : 2682000 \t\t Average Reward : 162.9\n",
            "Episode : 2028 \t\t Timestep : 2688000 \t\t Average Reward : 244.29\n",
            "Episode : 2034 \t\t Timestep : 2694000 \t\t Average Reward : 148.07\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2038 \t\t Timestep : 2700000 \t\t Average Reward : 247.55\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:51:13\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2042 \t\t Timestep : 2706000 \t\t Average Reward : 251.76\n",
            "Episode : 2046 \t\t Timestep : 2712000 \t\t Average Reward : 245.77\n",
            "Episode : 2050 \t\t Timestep : 2718000 \t\t Average Reward : 251.78\n",
            "Episode : 2054 \t\t Timestep : 2724000 \t\t Average Reward : 253.44\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2058 \t\t Timestep : 2730000 \t\t Average Reward : 238.74\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:51:47\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2062 \t\t Timestep : 2736000 \t\t Average Reward : 244.33\n",
            "Episode : 2067 \t\t Timestep : 2742000 \t\t Average Reward : 185.79\n",
            "Episode : 2071 \t\t Timestep : 2748000 \t\t Average Reward : 162.61\n",
            "Episode : 2076 \t\t Timestep : 2754000 \t\t Average Reward : 230.31\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2080 \t\t Timestep : 2760000 \t\t Average Reward : 245.87\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:52:21\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2084 \t\t Timestep : 2766000 \t\t Average Reward : 252.47\n",
            "Episode : 2088 \t\t Timestep : 2772000 \t\t Average Reward : 254.77\n",
            "Episode : 2092 \t\t Timestep : 2778000 \t\t Average Reward : 259.01\n",
            "Episode : 2096 \t\t Timestep : 2784000 \t\t Average Reward : 252.45\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2100 \t\t Timestep : 2790000 \t\t Average Reward : 251.24\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:52:55\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2105 \t\t Timestep : 2796000 \t\t Average Reward : 176.68\n",
            "Episode : 2109 \t\t Timestep : 2802000 \t\t Average Reward : 258.04\n",
            "Episode : 2113 \t\t Timestep : 2808000 \t\t Average Reward : 256.52\n",
            "Episode : 2117 \t\t Timestep : 2814000 \t\t Average Reward : 216.87\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2121 \t\t Timestep : 2820000 \t\t Average Reward : 199.61\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:53:29\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2125 \t\t Timestep : 2826000 \t\t Average Reward : 258.72\n",
            "Episode : 2129 \t\t Timestep : 2832000 \t\t Average Reward : 260.14\n",
            "Episode : 2133 \t\t Timestep : 2838000 \t\t Average Reward : 259.6\n",
            "Episode : 2138 \t\t Timestep : 2844000 \t\t Average Reward : 221.57\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2143 \t\t Timestep : 2850000 \t\t Average Reward : 181.17\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:54:03\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2147 \t\t Timestep : 2856000 \t\t Average Reward : 214.07\n",
            "Episode : 2152 \t\t Timestep : 2862000 \t\t Average Reward : 185.68\n",
            "Episode : 2156 \t\t Timestep : 2868000 \t\t Average Reward : 216.34\n",
            "Episode : 2161 \t\t Timestep : 2874000 \t\t Average Reward : 195.8\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2165 \t\t Timestep : 2880000 \t\t Average Reward : 222.12\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:54:37\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2169 \t\t Timestep : 2886000 \t\t Average Reward : 182.48\n",
            "Episode : 2173 \t\t Timestep : 2892000 \t\t Average Reward : 259.97\n",
            "Episode : 2177 \t\t Timestep : 2898000 \t\t Average Reward : 255.54\n",
            "Episode : 2181 \t\t Timestep : 2904000 \t\t Average Reward : 257.4\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2186 \t\t Timestep : 2910000 \t\t Average Reward : 183.59\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:55:11\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2190 \t\t Timestep : 2916000 \t\t Average Reward : 260.23\n",
            "Episode : 2194 \t\t Timestep : 2922000 \t\t Average Reward : 263.53\n",
            "Episode : 2198 \t\t Timestep : 2928000 \t\t Average Reward : 267.8\n",
            "Episode : 2204 \t\t Timestep : 2934000 \t\t Average Reward : 158.81\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2208 \t\t Timestep : 2940000 \t\t Average Reward : 222.39\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:55:46\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2212 \t\t Timestep : 2946000 \t\t Average Reward : 263.08\n",
            "Episode : 2217 \t\t Timestep : 2952000 \t\t Average Reward : 203.88\n",
            "Episode : 2222 \t\t Timestep : 2958000 \t\t Average Reward : 177.62\n",
            "Episode : 2227 \t\t Timestep : 2964000 \t\t Average Reward : 151.51\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2231 \t\t Timestep : 2970000 \t\t Average Reward : 264.88\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:56:20\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2235 \t\t Timestep : 2976000 \t\t Average Reward : 234.72\n",
            "Episode : 2240 \t\t Timestep : 2982000 \t\t Average Reward : 166.8\n",
            "Episode : 2244 \t\t Timestep : 2988000 \t\t Average Reward : 264.71\n",
            "Episode : 2249 \t\t Timestep : 2994000 \t\t Average Reward : 214.08\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2254 \t\t Timestep : 3000000 \t\t Average Reward : 130.41\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:56:54\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2258 \t\t Timestep : 3006000 \t\t Average Reward : 267.85\n",
            "Episode : 2262 \t\t Timestep : 3012000 \t\t Average Reward : 267.12\n",
            "Episode : 2266 \t\t Timestep : 3018000 \t\t Average Reward : 264.7\n",
            "Episode : 2271 \t\t Timestep : 3024000 \t\t Average Reward : 268.49\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2275 \t\t Timestep : 3030000 \t\t Average Reward : 268.42\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:57:28\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2281 \t\t Timestep : 3036000 \t\t Average Reward : 104.54\n",
            "Episode : 2288 \t\t Timestep : 3042000 \t\t Average Reward : 88.38\n",
            "Episode : 2294 \t\t Timestep : 3048000 \t\t Average Reward : 141.48\n",
            "Episode : 2299 \t\t Timestep : 3054000 \t\t Average Reward : 117.86\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2305 \t\t Timestep : 3060000 \t\t Average Reward : 136.31\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:58:01\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2309 \t\t Timestep : 3066000 \t\t Average Reward : 267.42\n",
            "Episode : 2313 \t\t Timestep : 3072000 \t\t Average Reward : 264.68\n",
            "Episode : 2317 \t\t Timestep : 3078000 \t\t Average Reward : 172.9\n",
            "Episode : 2321 \t\t Timestep : 3084000 \t\t Average Reward : 265.23\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2325 \t\t Timestep : 3090000 \t\t Average Reward : 264.74\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:58:35\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2330 \t\t Timestep : 3096000 \t\t Average Reward : 168.0\n",
            "Episode : 2335 \t\t Timestep : 3102000 \t\t Average Reward : 220.37\n",
            "Episode : 2339 \t\t Timestep : 3108000 \t\t Average Reward : 169.96\n",
            "Episode : 2343 \t\t Timestep : 3114000 \t\t Average Reward : 262.27\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2348 \t\t Timestep : 3120000 \t\t Average Reward : 230.07\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:59:09\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2352 \t\t Timestep : 3126000 \t\t Average Reward : 261.84\n",
            "Episode : 2356 \t\t Timestep : 3132000 \t\t Average Reward : 220.54\n",
            "Episode : 2360 \t\t Timestep : 3138000 \t\t Average Reward : 195.24\n",
            "Episode : 2364 \t\t Timestep : 3144000 \t\t Average Reward : 264.53\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2369 \t\t Timestep : 3150000 \t\t Average Reward : 197.51\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:59:43\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2373 \t\t Timestep : 3156000 \t\t Average Reward : 261.06\n",
            "Episode : 2377 \t\t Timestep : 3162000 \t\t Average Reward : 263.88\n",
            "Episode : 2381 \t\t Timestep : 3168000 \t\t Average Reward : 261.4\n",
            "Episode : 2386 \t\t Timestep : 3174000 \t\t Average Reward : 203.42\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2391 \t\t Timestep : 3180000 \t\t Average Reward : 194.61\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:00:18\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2396 \t\t Timestep : 3186000 \t\t Average Reward : 194.63\n",
            "Episode : 2400 \t\t Timestep : 3192000 \t\t Average Reward : 218.06\n",
            "Episode : 2405 \t\t Timestep : 3198000 \t\t Average Reward : 213.0\n",
            "Episode : 2409 \t\t Timestep : 3204000 \t\t Average Reward : 265.53\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2413 \t\t Timestep : 3210000 \t\t Average Reward : 265.32\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:00:52\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2417 \t\t Timestep : 3216000 \t\t Average Reward : 199.96\n",
            "Episode : 2421 \t\t Timestep : 3222000 \t\t Average Reward : 256.68\n",
            "Episode : 2425 \t\t Timestep : 3228000 \t\t Average Reward : 262.79\n",
            "Episode : 2430 \t\t Timestep : 3234000 \t\t Average Reward : 198.99\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2434 \t\t Timestep : 3240000 \t\t Average Reward : 260.52\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:01:26\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2439 \t\t Timestep : 3246000 \t\t Average Reward : 158.34\n",
            "Episode : 2443 \t\t Timestep : 3252000 \t\t Average Reward : 262.35\n",
            "Episode : 2447 \t\t Timestep : 3258000 \t\t Average Reward : 246.73\n",
            "Episode : 2451 \t\t Timestep : 3264000 \t\t Average Reward : 265.49\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2455 \t\t Timestep : 3270000 \t\t Average Reward : 262.07\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:02:01\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2459 \t\t Timestep : 3276000 \t\t Average Reward : 259.35\n",
            "Episode : 2463 \t\t Timestep : 3282000 \t\t Average Reward : 205.75\n",
            "Episode : 2468 \t\t Timestep : 3288000 \t\t Average Reward : 237.2\n",
            "Episode : 2472 \t\t Timestep : 3294000 \t\t Average Reward : 196.68\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2476 \t\t Timestep : 3300000 \t\t Average Reward : 260.01\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:02:35\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2480 \t\t Timestep : 3306000 \t\t Average Reward : 265.65\n",
            "Episode : 2484 \t\t Timestep : 3312000 \t\t Average Reward : 261.51\n",
            "Episode : 2489 \t\t Timestep : 3318000 \t\t Average Reward : 189.7\n",
            "Episode : 2494 \t\t Timestep : 3324000 \t\t Average Reward : 222.16\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2498 \t\t Timestep : 3330000 \t\t Average Reward : 196.74\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:03:09\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2503 \t\t Timestep : 3336000 \t\t Average Reward : 188.81\n",
            "Episode : 2508 \t\t Timestep : 3342000 \t\t Average Reward : 90.13\n",
            "Episode : 2512 \t\t Timestep : 3348000 \t\t Average Reward : 215.25\n",
            "Episode : 2516 \t\t Timestep : 3354000 \t\t Average Reward : 254.48\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2520 \t\t Timestep : 3360000 \t\t Average Reward : 265.36\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:03:44\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2525 \t\t Timestep : 3366000 \t\t Average Reward : 208.35\n",
            "Episode : 2529 \t\t Timestep : 3372000 \t\t Average Reward : 257.91\n",
            "Episode : 2533 \t\t Timestep : 3378000 \t\t Average Reward : 265.02\n",
            "Episode : 2537 \t\t Timestep : 3384000 \t\t Average Reward : 266.55\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2541 \t\t Timestep : 3390000 \t\t Average Reward : 265.23\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:04:18\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2545 \t\t Timestep : 3396000 \t\t Average Reward : 267.5\n",
            "Episode : 2549 \t\t Timestep : 3402000 \t\t Average Reward : 268.19\n",
            "Episode : 2554 \t\t Timestep : 3408000 \t\t Average Reward : 192.62\n",
            "Episode : 2558 \t\t Timestep : 3414000 \t\t Average Reward : 263.57\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2563 \t\t Timestep : 3420000 \t\t Average Reward : 211.12\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:04:52\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2567 \t\t Timestep : 3426000 \t\t Average Reward : 268.51\n",
            "Episode : 2572 \t\t Timestep : 3432000 \t\t Average Reward : 193.39\n",
            "Episode : 2576 \t\t Timestep : 3438000 \t\t Average Reward : 235.76\n",
            "Episode : 2580 \t\t Timestep : 3444000 \t\t Average Reward : 225.69\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2584 \t\t Timestep : 3450000 \t\t Average Reward : 265.1\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:05:26\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2588 \t\t Timestep : 3456000 \t\t Average Reward : 266.86\n",
            "Episode : 2592 \t\t Timestep : 3462000 \t\t Average Reward : 231.72\n",
            "Episode : 2596 \t\t Timestep : 3468000 \t\t Average Reward : 267.3\n",
            "Episode : 2600 \t\t Timestep : 3474000 \t\t Average Reward : 266.81\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2606 \t\t Timestep : 3480000 \t\t Average Reward : 143.69\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:06:00\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2610 \t\t Timestep : 3486000 \t\t Average Reward : 262.8\n",
            "Episode : 2615 \t\t Timestep : 3492000 \t\t Average Reward : 194.82\n",
            "Episode : 2619 \t\t Timestep : 3498000 \t\t Average Reward : 272.3\n",
            "Episode : 2623 \t\t Timestep : 3504000 \t\t Average Reward : 270.94\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2627 \t\t Timestep : 3510000 \t\t Average Reward : 264.56\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:06:34\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2632 \t\t Timestep : 3516000 \t\t Average Reward : 264.29\n",
            "Episode : 2636 \t\t Timestep : 3522000 \t\t Average Reward : 270.84\n",
            "Episode : 2640 \t\t Timestep : 3528000 \t\t Average Reward : 189.12\n",
            "Episode : 2644 \t\t Timestep : 3534000 \t\t Average Reward : 260.7\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2648 \t\t Timestep : 3540000 \t\t Average Reward : 265.1\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:07:08\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2653 \t\t Timestep : 3546000 \t\t Average Reward : 268.06\n",
            "Episode : 2658 \t\t Timestep : 3552000 \t\t Average Reward : 192.15\n",
            "Episode : 2662 \t\t Timestep : 3558000 \t\t Average Reward : 269.56\n",
            "Episode : 2666 \t\t Timestep : 3564000 \t\t Average Reward : 264.24\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2670 \t\t Timestep : 3570000 \t\t Average Reward : 268.39\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:07:43\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2675 \t\t Timestep : 3576000 \t\t Average Reward : 192.85\n",
            "Episode : 2679 \t\t Timestep : 3582000 \t\t Average Reward : 263.97\n",
            "Episode : 2683 \t\t Timestep : 3588000 \t\t Average Reward : 221.01\n",
            "Episode : 2687 \t\t Timestep : 3594000 \t\t Average Reward : 269.48\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2692 \t\t Timestep : 3600000 \t\t Average Reward : 192.83\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:08:17\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2697 \t\t Timestep : 3606000 \t\t Average Reward : 153.6\n",
            "Episode : 2701 \t\t Timestep : 3612000 \t\t Average Reward : 267.15\n",
            "Episode : 2705 \t\t Timestep : 3618000 \t\t Average Reward : 272.4\n",
            "Episode : 2709 \t\t Timestep : 3624000 \t\t Average Reward : 265.48\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2713 \t\t Timestep : 3630000 \t\t Average Reward : 272.15\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:08:51\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2717 \t\t Timestep : 3636000 \t\t Average Reward : 263.65\n",
            "Episode : 2721 \t\t Timestep : 3642000 \t\t Average Reward : 227.98\n",
            "Episode : 2726 \t\t Timestep : 3648000 \t\t Average Reward : 216.98\n",
            "Episode : 2730 \t\t Timestep : 3654000 \t\t Average Reward : 266.38\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2734 \t\t Timestep : 3660000 \t\t Average Reward : 269.52\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:09:25\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2738 \t\t Timestep : 3666000 \t\t Average Reward : 262.29\n",
            "Episode : 2743 \t\t Timestep : 3672000 \t\t Average Reward : 222.16\n",
            "Episode : 2747 \t\t Timestep : 3678000 \t\t Average Reward : 218.96\n",
            "Episode : 2751 \t\t Timestep : 3684000 \t\t Average Reward : 198.68\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2755 \t\t Timestep : 3690000 \t\t Average Reward : 266.27\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:09:59\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2759 \t\t Timestep : 3696000 \t\t Average Reward : 265.93\n",
            "Episode : 2764 \t\t Timestep : 3702000 \t\t Average Reward : 188.13\n",
            "Episode : 2769 \t\t Timestep : 3708000 \t\t Average Reward : 223.86\n",
            "Episode : 2773 \t\t Timestep : 3714000 \t\t Average Reward : 185.71\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2778 \t\t Timestep : 3720000 \t\t Average Reward : 195.22\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:10:33\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2782 \t\t Timestep : 3726000 \t\t Average Reward : 266.42\n",
            "Episode : 2786 \t\t Timestep : 3732000 \t\t Average Reward : 255.87\n",
            "Episode : 2791 \t\t Timestep : 3738000 \t\t Average Reward : 211.69\n",
            "Episode : 2796 \t\t Timestep : 3744000 \t\t Average Reward : 204.45\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2800 \t\t Timestep : 3750000 \t\t Average Reward : 266.62\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:11:07\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2804 \t\t Timestep : 3756000 \t\t Average Reward : 214.22\n",
            "Episode : 2808 \t\t Timestep : 3762000 \t\t Average Reward : 223.25\n",
            "Episode : 2813 \t\t Timestep : 3768000 \t\t Average Reward : 184.8\n",
            "Episode : 2817 \t\t Timestep : 3774000 \t\t Average Reward : 267.53\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2821 \t\t Timestep : 3780000 \t\t Average Reward : 259.56\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:11:41\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2825 \t\t Timestep : 3786000 \t\t Average Reward : 229.9\n",
            "Episode : 2829 \t\t Timestep : 3792000 \t\t Average Reward : 264.63\n",
            "Episode : 2835 \t\t Timestep : 3798000 \t\t Average Reward : 119.68\n",
            "Episode : 2840 \t\t Timestep : 3804000 \t\t Average Reward : 200.37\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2845 \t\t Timestep : 3810000 \t\t Average Reward : 124.88\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:12:16\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2849 \t\t Timestep : 3816000 \t\t Average Reward : 263.57\n",
            "Episode : 2854 \t\t Timestep : 3822000 \t\t Average Reward : 157.24\n",
            "Episode : 2859 \t\t Timestep : 3828000 \t\t Average Reward : 203.87\n",
            "Episode : 2863 \t\t Timestep : 3834000 \t\t Average Reward : 260.2\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2867 \t\t Timestep : 3840000 \t\t Average Reward : 264.12\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:12:50\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2872 \t\t Timestep : 3846000 \t\t Average Reward : 225.24\n",
            "Episode : 2877 \t\t Timestep : 3852000 \t\t Average Reward : 106.1\n",
            "Episode : 2881 \t\t Timestep : 3858000 \t\t Average Reward : 267.22\n",
            "Episode : 2885 \t\t Timestep : 3864000 \t\t Average Reward : 259.22\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2890 \t\t Timestep : 3870000 \t\t Average Reward : 179.46\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:13:24\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2895 \t\t Timestep : 3876000 \t\t Average Reward : 187.97\n",
            "Episode : 2899 \t\t Timestep : 3882000 \t\t Average Reward : 229.57\n",
            "Episode : 2904 \t\t Timestep : 3888000 \t\t Average Reward : 207.15\n",
            "Episode : 2908 \t\t Timestep : 3894000 \t\t Average Reward : 268.45\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2913 \t\t Timestep : 3900000 \t\t Average Reward : 187.51\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:13:58\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2917 \t\t Timestep : 3906000 \t\t Average Reward : 251.68\n",
            "Episode : 2921 \t\t Timestep : 3912000 \t\t Average Reward : 206.28\n",
            "Episode : 2925 \t\t Timestep : 3918000 \t\t Average Reward : 212.53\n",
            "Episode : 2929 \t\t Timestep : 3924000 \t\t Average Reward : 260.65\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2933 \t\t Timestep : 3930000 \t\t Average Reward : 264.32\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:14:32\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2938 \t\t Timestep : 3936000 \t\t Average Reward : 192.59\n",
            "Episode : 2943 \t\t Timestep : 3942000 \t\t Average Reward : 188.08\n",
            "Episode : 2947 \t\t Timestep : 3948000 \t\t Average Reward : 250.05\n",
            "Episode : 2951 \t\t Timestep : 3954000 \t\t Average Reward : 264.73\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2955 \t\t Timestep : 3960000 \t\t Average Reward : 255.66\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:15:06\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2959 \t\t Timestep : 3966000 \t\t Average Reward : 231.77\n",
            "Episode : 2964 \t\t Timestep : 3972000 \t\t Average Reward : 195.11\n",
            "Episode : 2969 \t\t Timestep : 3978000 \t\t Average Reward : 203.24\n",
            "Episode : 2973 \t\t Timestep : 3984000 \t\t Average Reward : 230.17\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2977 \t\t Timestep : 3990000 \t\t Average Reward : 269.31\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  1:15:40\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2981 \t\t Timestep : 3996000 \t\t Average Reward : 210.6\n",
            "============================================================================================\n",
            "Started training at (GMT) :  2022-02-28 13:43:19\n",
            "Finished training at (GMT) :  2022-02-28 14:59:11\n",
            "Total training time  :  1:15:52\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "################################### Training ###################################\n",
        "\n",
        "\n",
        "####### initialize environment hyperparameters ######\n",
        "\n",
        "env_name = \"BipedalWalker-v2\"\n",
        "has_continuous_action_space = True\n",
        "max_ep_len = 1500           # max timesteps in one episode\n",
        "action_std = 0.8           # set same std for action distribution which was used while saving\n",
        "action_std_decay_rate = 0.01\n",
        "min_action_std = 0.3\n",
        "action_std_decay_freq = 30000\n",
        "\n",
        "max_training_timesteps = int(4e6)   # break training loop if timeteps > max_training_timesteps\n",
        "\n",
        "print_freq = max_ep_len * 4     # print avg reward in the interval (in num timesteps)\n",
        "log_freq = max_ep_len * 2       # log avg reward in the interval (in num timesteps)\n",
        "save_model_freq = int(3e4)      # save model frequency (in num timesteps)\n",
        "\n",
        "\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "## Note : print/log frequencies should be > than max_ep_len\n",
        "\n",
        "\n",
        "################ PPO hyperparameters ################\n",
        "\n",
        "\n",
        "update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
        "K_epochs = 40               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.99                # discount factor\n",
        "\n",
        "lr_actor = 0.0003       # learning rate for actor network\n",
        "lr_critic = 0.001       # learning rate for critic network\n",
        "\n",
        "random_seed = 0         # set random seed if required (0 = no random seed)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "\n",
        "print(\"training environment name : \" + env_name)\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# action space dimension\n",
        "if has_continuous_action_space:\n",
        "    action_dim = env.action_space.shape[0]\n",
        "else:\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "\n",
        "###################### logging ######################\n",
        "\n",
        "#### log files for multiple runs are NOT overwritten\n",
        "\n",
        "log_dir = \"PPO_logs\"\n",
        "if not os.path.exists(log_dir):\n",
        "      os.makedirs(log_dir)\n",
        "\n",
        "log_dir = log_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(log_dir):\n",
        "      os.makedirs(log_dir)\n",
        "\n",
        "\n",
        "#### get number of log files in log directory\n",
        "run_num = 0\n",
        "current_num_files = next(os.walk(log_dir))[2]\n",
        "run_num = len(current_num_files)\n",
        "\n",
        "\n",
        "#### create new log file for each run \n",
        "log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
        "\n",
        "print(\"current logging run number for \" + env_name + \" : \", run_num)\n",
        "print(\"logging at : \" + log_f_name)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "################### checkpointing ###################\n",
        "\n",
        "run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
        "\n",
        "directory = \"PPO_preTrained\"\n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "directory = directory + '/' + env_name + '/'\n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"save checkpoint path : \" + checkpoint_path)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "############# print all hyperparameters #############\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"max training timesteps : \", max_training_timesteps)\n",
        "print(\"max timesteps per episode : \", max_ep_len)\n",
        "\n",
        "print(\"model saving frequency : \" + str(save_model_freq) + \" timesteps\")\n",
        "print(\"log frequency : \" + str(log_freq) + \" timesteps\")\n",
        "print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"state space dimension : \", state_dim)\n",
        "print(\"action space dimension : \", action_dim)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "if has_continuous_action_space:\n",
        "    print(\"Initializing a continuous action space policy\")\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "    print(\"starting std of action distribution : \", action_std)\n",
        "    print(\"decay rate of std of action distribution : \", action_std_decay_rate)\n",
        "    print(\"minimum std of action distribution : \", min_action_std)\n",
        "    print(\"decay frequency of std of action distribution : \" + str(action_std_decay_freq) + \" timesteps\")\n",
        "\n",
        "else:\n",
        "    print(\"Initializing a discrete action space policy\")\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\") \n",
        "print(\"PPO K epochs : \", K_epochs)\n",
        "print(\"PPO epsilon clip : \", eps_clip)\n",
        "print(\"discount factor (gamma) : \", gamma)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"optimizer learning rate actor : \", lr_actor)\n",
        "print(\"optimizer learning rate critic : \", lr_critic)\n",
        "\n",
        "if random_seed:\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "    print(\"setting random seed to \", random_seed)\n",
        "    torch.manual_seed(random_seed)\n",
        "    env.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "################# training procedure ################\n",
        "\n",
        "# initialize a PPO agent\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "\n",
        "# track total training time\n",
        "start_time = datetime.now().replace(microsecond=0)\n",
        "print(\"Started training at (GMT) : \", start_time)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "# logging file\n",
        "log_f = open(log_f_name,\"w+\")\n",
        "log_f.write('episode,timestep,reward\\n')\n",
        "\n",
        "\n",
        "# printing and logging variables\n",
        "print_running_reward = 0\n",
        "print_running_episodes = 0\n",
        "\n",
        "log_running_reward = 0\n",
        "log_running_episodes = 0\n",
        "\n",
        "time_step = 0\n",
        "i_episode = 0\n",
        "\n",
        "\n",
        "# training loop\n",
        "while time_step <= max_training_timesteps:\n",
        "    \n",
        "    state = env.reset()\n",
        "    current_ep_reward = 0\n",
        "\n",
        "    for t in range(1, max_ep_len+1):\n",
        "        \n",
        "        # select action with policy\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        \n",
        "        # saving reward and is_terminals\n",
        "        ppo_agent.buffer.rewards.append(reward)\n",
        "        ppo_agent.buffer.is_terminals.append(done)\n",
        "        \n",
        "        time_step +=1\n",
        "        current_ep_reward += reward\n",
        "\n",
        "        # update PPO agent\n",
        "        if time_step % update_timestep == 0:\n",
        "            ppo_agent.update()\n",
        "\n",
        "        # if continuous action space; then decay action std of ouput action distribution\n",
        "        if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
        "            ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
        "\n",
        "        # log in logging file\n",
        "        if time_step % log_freq == 0:\n",
        "\n",
        "            # log average reward till last episode\n",
        "            log_avg_reward = log_running_reward / log_running_episodes\n",
        "            log_avg_reward = round(log_avg_reward, 4)\n",
        "\n",
        "            log_f.write('{},{},{}\\n'.format(i_episode, time_step, log_avg_reward))\n",
        "            log_f.flush()\n",
        "\n",
        "            log_running_reward = 0\n",
        "            log_running_episodes = 0\n",
        "\n",
        "        # printing average reward\n",
        "        if time_step % print_freq == 0:\n",
        "\n",
        "            # print average reward till last episode\n",
        "            print_avg_reward = print_running_reward / print_running_episodes\n",
        "            print_avg_reward = round(print_avg_reward, 2)\n",
        "\n",
        "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
        "\n",
        "            print_running_reward = 0\n",
        "            print_running_episodes = 0\n",
        "            \n",
        "        # save model weights\n",
        "        if time_step % save_model_freq == 0:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"saving model at : \" + checkpoint_path)\n",
        "            ppo_agent.save(checkpoint_path)\n",
        "            print(\"model saved\")\n",
        "            print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            \n",
        "        # break; if the episode is over\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print_running_reward += current_ep_reward\n",
        "    print_running_episodes += 1\n",
        "\n",
        "    log_running_reward += current_ep_reward\n",
        "    log_running_episodes += 1\n",
        "\n",
        "    i_episode += 1\n",
        "\n",
        "\n",
        "log_f.close()\n",
        "env.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print total training time\n",
        "print(\"============================================================================================\")\n",
        "end_time = datetime.now().replace(microsecond=0)\n",
        "print(\"Started training at (GMT) : \", start_time)\n",
        "print(\"Finished training at (GMT) : \", end_time)\n",
        "print(\"Total training time  : \", end_time - start_time)\n",
        "print(\"============================================================================================\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHhK13_1G6zX"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - III**\n",
        "\n",
        "*   load and test preTrained networks on environments\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SZWyhkq9Gxm5",
        "outputId": "48def58e-b670-4d34-e691-6f7ba21c77ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================================================\n",
            "loading network from : PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode: 1 \t\t Reward: 284.52\n",
            "Episode: 2 \t\t Reward: 281.23\n",
            "Episode: 3 \t\t Reward: 280.34\n",
            "Episode: 4 \t\t Reward: 282.41\n",
            "Episode: 5 \t\t Reward: 285.08\n",
            "Episode: 6 \t\t Reward: 285.5\n",
            "Episode: 7 \t\t Reward: 278.72\n",
            "Episode: 8 \t\t Reward: 279.84\n",
            "Episode: 9 \t\t Reward: 283.1\n",
            "Episode: 10 \t\t Reward: 286.92\n",
            "============================================================================================\n",
            "average test reward : 282.77\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "#################################### Testing ###################################\n",
        "\n",
        "\n",
        "################## hyperparameters ##################\n",
        "\n",
        "\n",
        "env_name = \"BipedalWalker-v2\"\n",
        "has_continuous_action_space = True\n",
        "max_ep_len = 3000           # max timesteps in one episode\n",
        "action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "\n",
        "\n",
        "total_test_episodes = 10    # total num of testing episodes\n",
        "\n",
        "K_epochs = 80               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.99                # discount factor\n",
        "\n",
        "lr_actor = 0.0003           # learning rate for actor\n",
        "lr_critic = 0.001           # learning rate for critic\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# action space dimension\n",
        "if has_continuous_action_space:\n",
        "    action_dim = env.action_space.shape[0]\n",
        "else:\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "# initialize a PPO agent\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "\n",
        "# preTrained weights directory\n",
        "\n",
        "random_seed = 0             #### set this to load a particular checkpoint trained on random seed\n",
        "run_num_pretrained = 0      #### set this to load a particular checkpoint num\n",
        "\n",
        "\n",
        "directory = \"PPO_preTrained\" + '/' + env_name + '/'\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"loading network from : \" + checkpoint_path)\n",
        "\n",
        "ppo_agent.load(checkpoint_path)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "test_running_reward = 0\n",
        "\n",
        "for ep in range(1, total_test_episodes+1):\n",
        "    ep_reward = 0\n",
        "    state = env.reset()\n",
        "    \n",
        "    for t in range(1, max_ep_len+1):\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        ep_reward += reward\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # clear buffer    \n",
        "    ppo_agent.buffer.clear()\n",
        "\n",
        "    test_running_reward +=  ep_reward\n",
        "    print('Episode: {} \\t\\t Reward: {}'.format(ep, round(ep_reward, 2)))\n",
        "    ep_reward = 0\n",
        "\n",
        "env.close()\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "avg_test_reward = test_running_reward / total_test_episodes\n",
        "avg_test_reward = round(avg_test_reward, 2)\n",
        "print(\"average test reward : \" + str(avg_test_reward))\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZewQELovHFt4"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - IV**\n",
        "\n",
        "*   load log files using pandas\n",
        "*   plot graph using matplotlib\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bY-E5HGcGxiu",
        "outputId": "91ddd10c-248c-499d-f5b7-b56923447a7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================================================\n",
            "loading data from : PPO_logs/BipedalWalker-v2//PPO_BipedalWalker-v2_log_0.csv\n",
            "data shape :  (1333, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "============================================================================================\n",
            "figure saved at :  PPO_figs/BipedalWalker-v2//PPO_BipedalWalker-v2_fig_0.png\n",
            "============================================================================================\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "env_name = 'BipedalWalker-v2'\n",
        "\n",
        "\n",
        "\n",
        "fig_num = 0     #### change this to prevent overwriting figures in same env_name folder\n",
        "\n",
        "plot_avg = True    # plot average of all runs; else plot all runs separately\n",
        "\n",
        "fig_width = 10\n",
        "fig_height = 6\n",
        "\n",
        "\n",
        "# smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "window_len_smooth = 50\n",
        "min_window_len_smooth = 1\n",
        "linewidth_smooth = 1.5\n",
        "alpha_smooth = 1\n",
        "\n",
        "window_len_var = 5\n",
        "min_window_len_var = 1\n",
        "linewidth_var = 2\n",
        "alpha_var = 0.1\n",
        "\n",
        "\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple', 'olive', 'brown', 'magenta', 'cyan', 'crimson','gray', 'black']\n",
        "\n",
        "\n",
        "# make directory for saving figures\n",
        "figures_dir = \"PPO_figs\"\n",
        "if not os.path.exists(figures_dir):\n",
        "    os.makedirs(figures_dir)\n",
        "\n",
        "# make environment directory for saving figures\n",
        "figures_dir = figures_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(figures_dir):\n",
        "    os.makedirs(figures_dir)\n",
        "\n",
        "\n",
        "fig_save_path = figures_dir + '/PPO_' + env_name + '_fig_' + str(fig_num) + '.png'\n",
        "\n",
        "\n",
        "# get number of log files in directory\n",
        "log_dir = \"PPO_logs\" + '/' + env_name + '/'\n",
        "\n",
        "current_num_files = next(os.walk(log_dir))[2]\n",
        "num_runs = len(current_num_files)\n",
        "\n",
        "\n",
        "all_runs = []\n",
        "\n",
        "for run_num in range(num_runs):\n",
        "\n",
        "    log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
        "    print(\"loading data from : \" + log_f_name)\n",
        "    data = pd.read_csv(log_f_name)\n",
        "    data = pd.DataFrame(data)\n",
        "    \n",
        "    print(\"data shape : \", data.shape)\n",
        "    \n",
        "    all_runs.append(data)\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "if plot_avg:\n",
        "    # average all runs\n",
        "    df_concat = pd.concat(all_runs)\n",
        "    df_concat_groupby = df_concat.groupby(df_concat.index)\n",
        "    data_avg = df_concat_groupby.mean()\n",
        "\n",
        "    # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "    data_avg['reward_smooth'] = data_avg['reward'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
        "    data_avg['reward_var'] = data_avg['reward'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
        "\n",
        "    data_avg.plot(kind='line', x='timestep' , y='reward_smooth',ax=ax,color=colors[0],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
        "    data_avg.plot(kind='line', x='timestep' , y='reward_var',ax=ax,color=colors[0],  linewidth=linewidth_var, alpha=alpha_var)\n",
        "\n",
        "    # keep only reward_smooth in the legend and rename it\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    ax.legend([handles[0]], [\"reward_avg_\" + str(len(all_runs)) + \"_runs\"], loc=2)\n",
        "\n",
        "\n",
        "else:\n",
        "    for i, run in enumerate(all_runs):\n",
        "        # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "        run['reward_smooth_' + str(i)] = run['reward'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
        "        run['reward_var_' + str(i)] = run['reward'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
        "        \n",
        "        # plot the lines\n",
        "        run.plot(kind='line', x='timestep' , y='reward_smooth_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
        "        run.plot(kind='line', x='timestep' , y='reward_var_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_var, alpha=alpha_var)\n",
        "\n",
        "    # keep alternate elements (reward_smooth_i) in the legend\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    new_handles = []\n",
        "    new_labels = []\n",
        "    for i in range(len(handles)):\n",
        "        if(i%2 == 0):\n",
        "            new_handles.append(handles[i])\n",
        "            new_labels.append(labels[i])\n",
        "    ax.legend(new_handles, new_labels, loc=2)\n",
        "\n",
        "\n",
        "\n",
        "# ax.set_yticks(np.arange(0, 1800, 200))\n",
        "# ax.set_xticks(np.arange(0, int(4e6), int(5e5)))\n",
        "\n",
        "\n",
        "ax.grid(color='gray', linestyle='-', linewidth=1, alpha=0.2)\n",
        "\n",
        "ax.set_xlabel(\"Timesteps\", fontsize=12)\n",
        "ax.set_ylabel(\"Rewards\", fontsize=12)\n",
        "\n",
        "plt.title(env_name, fontsize=14)\n",
        "\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(fig_width, fig_height)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "plt.savefig(fig_save_path)\n",
        "print(\"figure saved at : \", fig_save_path)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8uG43MtHNGC"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - V**\n",
        "\n",
        "*   install virtual display libraries for rendering on colab / remote server ^\n",
        "*   load preTrained networks and save images for gif\n",
        "*   generate and save gif from previously saved images\n",
        "\n",
        "*   ^ If running locally; do not install xvbf and pyvirtualdisplay. Just comment out the virtual display code and render it normally. \n",
        "*   ^ You will still require to use ipythondisplay, if you want to render it in the Jupyter Notebook.\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VL3tpKf3HLAq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#### to render on colab / server / headless machine install virtual display libraries\n",
        "\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5Rx_IFKHK-D"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "############################# save images for gif ##############################\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import gym\n",
        "import roboschool\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "One frame corresponding to each timestep is saved in a folder :\n",
        "\n",
        "PPO_gif_images/env_name/000001.jpg\n",
        "PPO_gif_images/env_name/000002.jpg\n",
        "PPO_gif_images/env_name/000003.jpg\n",
        "...\n",
        "...\n",
        "...\n",
        "\n",
        "\n",
        "if this section is run multiple times or for multiple episodes for the same env_name; \n",
        "then the saved images will be overwritten.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### beginning of virtual display code section\n",
        "\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "#### end of virtual display code section\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "################## hyperparameters ##################\n",
        "\n",
        "env_name = \"BipedalWalker-v2\"\n",
        "has_continuous_action_space = True\n",
        "max_ep_len = 1500           # max timesteps in one episode\n",
        "action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "\n",
        "\n",
        "total_test_episodes = 1     # save gif for only one episode\n",
        "\n",
        "render_ipython = False      # plot the images using matplotlib and ipythondisplay before saving (slow)\n",
        "\n",
        "K_epochs = 80               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.99                # discount factor\n",
        "\n",
        "lr_actor = 0.0003         # learning rate for actor\n",
        "lr_critic = 0.001         # learning rate for critic\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# action space dimension\n",
        "if has_continuous_action_space:\n",
        "    action_dim = env.action_space.shape[0]\n",
        "else:\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "\n",
        "# make directory for saving gif images\n",
        "gif_images_dir = \"PPO_gif_images\" + '/'\n",
        "if not os.path.exists(gif_images_dir):\n",
        "    os.makedirs(gif_images_dir)\n",
        "\n",
        "# make environment directory for saving gif images\n",
        "gif_images_dir = gif_images_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(gif_images_dir):\n",
        "    os.makedirs(gif_images_dir)\n",
        "\n",
        "# make directory for gif\n",
        "gif_dir = \"PPO_gifs\" + '/'\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "# make environment directory for gif\n",
        "gif_dir = gif_dir + '/' + env_name  + '/'\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "\n",
        "\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "\n",
        "# preTrained weights directory\n",
        "\n",
        "random_seed = 0             #### set this to load a particular checkpoint trained on random seed\n",
        "run_num_pretrained = 0      #### set this to load a particular checkpoint num\n",
        "\n",
        "\n",
        "directory = \"PPO_preTrained\" + '/' + env_name + '/'\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"loading network from : \" + checkpoint_path)\n",
        "\n",
        "ppo_agent.load(checkpoint_path)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "test_running_reward = 0\n",
        "\n",
        "for ep in range(1, total_test_episodes+1):\n",
        "    \n",
        "    ep_reward = 0\n",
        "    state = env.reset()\n",
        "\n",
        "    for t in range(1, max_ep_len+1):\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        ep_reward += reward\n",
        "\n",
        "        img = env.render(mode = 'rgb_array')\n",
        "\n",
        "\n",
        "        #### beginning of ipythondisplay code section 1\n",
        "\n",
        "        if render_ipython:\n",
        "            plt.imshow(img)\n",
        "            ipythondisplay.clear_output(wait=True)\n",
        "            ipythondisplay.display(plt.gcf())\n",
        "\n",
        "        #### end of ipythondisplay code section 1\n",
        "\n",
        "\n",
        "        img = Image.fromarray(img)\n",
        "        img.save(gif_images_dir + '/' + str(t).zfill(6) + '.jpg')\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    # clear buffer    \n",
        "    ppo_agent.buffer.clear()\n",
        "    \n",
        "    test_running_reward +=  ep_reward\n",
        "    print('Episode: {} \\t\\t Reward: {}'.format(ep, round(ep_reward, 2)))\n",
        "    ep_reward = 0\n",
        "\n",
        "\n",
        "\n",
        "env.close()\n",
        "\n",
        "\n",
        "#### beginning of ipythondisplay code section 2\n",
        "\n",
        "if render_ipython:\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "\n",
        "#### end of ipythondisplay code section 2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "print(\"total number of frames / timesteps / images saved : \", t)\n",
        "\n",
        "avg_test_reward = test_running_reward / total_test_episodes\n",
        "avg_test_reward = round(avg_test_reward, 2)\n",
        "print(\"average test reward : \" + str(avg_test_reward))\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BoVshl_ZHK7s",
        "outputId": "592e9bb2-ed49-4243-c91e-f06a56730a6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================================================\n",
            "total frames in gif :  30\n",
            "total duration of gif : 4.5 seconds\n",
            "saved gif at :  PPO_gifs/BipedalWalker-v2/PPO_BipedalWalker-v2_gif_0.gif\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "######################## generate gif from saved images ########################\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "env_name = 'BipedalWalker-v2'\n",
        "\n",
        "\n",
        "gif_num = 0     #### change this to prevent overwriting gifs in same env_name folder\n",
        "\n",
        "# adjust following parameters to get desired duration, size (bytes) and smoothness of gif\n",
        "total_timesteps = 300\n",
        "step = 10\n",
        "frame_duration = 150\n",
        "\n",
        "\n",
        "# input images\n",
        "gif_images_dir = \"PPO_gif_images/\" + env_name + '/*.jpg'\n",
        "\n",
        "\n",
        "# ouput gif path\n",
        "gif_dir = \"PPO_gifs\"\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "gif_dir = gif_dir + '/' + env_name\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "gif_path = gif_dir + '/PPO_' + env_name + '_gif_' + str(gif_num) + '.gif'\n",
        "\n",
        "\n",
        "\n",
        "img_paths = sorted(glob.glob(gif_images_dir))\n",
        "img_paths = img_paths[:total_timesteps]\n",
        "img_paths = img_paths[::step]\n",
        "\n",
        "\n",
        "print(\"total frames in gif : \", len(img_paths))\n",
        "print(\"total duration of gif : \" + str(round(len(img_paths) * frame_duration / 1000, 2)) + \" seconds\")\n",
        "\n",
        "\n",
        "\n",
        "# save gif\n",
        "img, *imgs = [Image.open(f) for f in img_paths]\n",
        "img.save(fp=gif_path, format='GIF', append_images=imgs, save_all=True, optimize=True, duration=frame_duration, loop=0)\n",
        "\n",
        "print(\"saved gif at : \", gif_path)\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20d1bR8xHK5j"
      },
      "outputs": [],
      "source": [
        "\n",
        "############################# check gif byte size ##############################\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "env_name = 'BipedalWalker-v2'\n",
        "\n",
        "\n",
        "\n",
        "gif_dir = \"PPO_gifs/\" + env_name + '/*.gif'\n",
        "\n",
        "gif_paths = sorted(glob.glob(gif_dir))\n",
        "\n",
        "for gif_path in gif_paths:\n",
        "    file_size = os.path.getsize(gif_path)\n",
        "    print(gif_path + '\\t\\t' + str(round(file_size / (1024 * 1024), 2)) + \" MB\")\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YUzQOu1HYHR"
      },
      "source": [
        "################################################################################\n",
        "\n",
        "---------------------------------------------------------------------------- That's all folks ! ----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "################################################################################"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Z4VJcUT2GlJz"
      ],
      "name": "Copy of PPO_colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}