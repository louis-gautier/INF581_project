{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-W6pdFHN8fJz",
        "outputId": "07f2a988-2347-4f52-bf3d-3a39e897cc51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.21.5)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.3.5)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.2->seaborn) (1.15.0)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (3.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio) (1.21.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio) (7.1.2)\n",
            "Requirement already satisfied: nnfigs in /usr/local/lib/python3.7/dist-packages (0.1.dev0)\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.10).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-470\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-470\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "python_libraries = [\"numpy\",\"pandas\",\"gym\",\"seaborn\",\"pyvirtualdisplay\",\"imageio\",\"nnfigs\",\"box2d-py\"]\n",
        "\n",
        "packages = [\"xvfb\",\"x11-utils\"]\n",
        "\n",
        "with open('script.sh', 'w') as file:\n",
        "  for library in python_libraries:\n",
        "    file.write(\"pip install \" + library +\"\\n\")\n",
        "  for package in packages:\n",
        "    file.write(\"apt install \" + package +\"\\n\")\n",
        "\n",
        "!bash script.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kVMo4E0o8UmO"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import numpy as np\n",
        "import copy\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import time\n",
        "import imageio\n",
        "import IPython\n",
        "from IPython.display import Image\n",
        "import pyvirtualdisplay\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import deque, namedtuple\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import utils\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
        "_ = _display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "16d9DXmAunK3"
      },
      "outputs": [],
      "source": [
        "# Code inspired by https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-bipedal and https://github.com/Jonathan-Pearce/DDPG_PER\n",
        "\n",
        "class Actor(torch.nn.Module):\n",
        "    def __init__(self, state_size, action_space, layer1_size=400, layer2_size=300):\n",
        "        super(Actor, self).__init__()\n",
        "        self.action_space = action_space\n",
        "        self.layer1 = torch.nn.Linear(state_size, layer1_size)\n",
        "        self.layer2 = torch.nn.Linear(layer1_size, layer2_size)\n",
        "        self.layer3 = torch.nn.Linear(layer2_size, action_space.shape[0])\n",
        "\n",
        "        # Initialization methods are taken as in the DDPG paper\n",
        "        bound1 = 1.0/np.sqrt(self.layer1.weight.data.size()[0])\n",
        "        nn.init.uniform_(self.layer1.weight.data, -bound1, bound1)\n",
        "        nn.init.uniform_(self.layer1.bias.data, -bound1, bound1)\n",
        "\n",
        "        bound2 = 1.0/np.sqrt(self.layer2.weight.data.size()[0])\n",
        "        nn.init.uniform_(self.layer2.weight.data, -bound2, bound2)\n",
        "        nn.init.uniform_(self.layer2.bias.data, -bound2, bound2)\n",
        "\n",
        "        bound3 = 0.003\n",
        "        nn.init.uniform_(self.layer3.weight.data, -bound3, bound3)\n",
        "        nn.init.uniform_(self.layer3.bias.data, -bound3, bound3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = self.layer3(x)\n",
        "        return torch.tanh(x)\n",
        "\n",
        "\n",
        "class Critic(torch.nn.Module):\n",
        "    def __init__(self, state_size, action_size, layer1_size=400, layer2_size=300):\n",
        "        super(Critic, self).__init__()\n",
        "        self.layer1 = torch.nn.Linear(state_size, layer1_size)\n",
        "        self.layer2 = torch.nn.Linear(layer1_size+action_size, layer2_size)\n",
        "        self.layer3 = torch.nn.Linear(layer2_size, 1)\n",
        "\n",
        "        # Initialization methods are taken as in the DDPG paper\n",
        "        bound1 = 1.0/np.sqrt(self.layer1.weight.data.size()[0])\n",
        "        nn.init.uniform_(self.layer1.weight.data, -bound1, bound1)\n",
        "        nn.init.uniform_(self.layer1.bias.data, -bound1, bound1)\n",
        "\n",
        "        bound2 = 1.0/np.sqrt(self.layer2.weight.data.size()[0])\n",
        "        nn.init.uniform_(self.layer2.weight.data, -bound2, bound2)\n",
        "        nn.init.uniform_(self.layer2.bias.data, -bound2, bound2)\n",
        "\n",
        "        bound3 = 0.0003\n",
        "        nn.init.uniform_(self.layer3.weight.data, -bound3, bound3)\n",
        "        nn.init.uniform_(self.layer3.bias.data, -bound3, bound3)\n",
        "\n",
        "\n",
        "    def forward(self, x, a):\n",
        "        layer1_out = F.relu(self.layer1(x))\n",
        "        layer2_out = F.relu(self.layer2(torch.cat([layer1_out, a], dim=1)))\n",
        "        return self.layer3(layer2_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CbAzKOWtITsS"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "buffer_size = int(1e6)  # Replay buffer size\n",
        "batch_size = 256        # Minibatch size\n",
        "gamma = 0.99            # Discount factor in gain function\n",
        "tau = 5e-3              # Update of target networks\n",
        "lr_actor = 1e-5         # Learning rate of actor neural network\n",
        "lr_critic = 1e-4        # Learning rate of critic neural network\n",
        "decay = 0.01            # Weight decay in Adam optimizer\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Agent():\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_size, action_size, action_space, random_seed):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(random_seed)\n",
        "\n",
        "        # Initialization of actor networks\n",
        "        self.actor_local = Actor(state_size, action_space).to(device)\n",
        "        self.actor_target = Actor(state_size, action_space).to(device)\n",
        "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=lr_actor)\n",
        "\n",
        "        # Initialization of critic networks\n",
        "        self.critic_local = Critic(state_size, action_size).to(device)\n",
        "        self.critic_target = Critic(state_size, action_size).to(device)\n",
        "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=lr_critic, weight_decay=decay)\n",
        "\n",
        "        # Noise process for action taking\n",
        "        self.noise = OUNoise(action_size, random_seed)\n",
        "\n",
        "        # Prioritized Experience Replay\n",
        "        self.memory = PrioritizedReplayBuffer(buffer_size, alpha=0.5)\n",
        "    \n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        # Save experience in memory\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        # If enough samples are available in memory, one iteration of the learning process for both the actor and the critic\n",
        "        if len(self.memory) > batch_size:\n",
        "            experiences = self.memory.sample(batch_size,beta=0.5)\n",
        "            self.learn(experiences, gamma)\n",
        "\n",
        "    def act(self, state, add_noise=True):\n",
        "        # Returns the best action as a function of the current policy\n",
        "        state = torch.from_numpy(state).float().to(device)\n",
        "        self.actor_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action = self.actor_local(state).cpu().data.numpy()\n",
        "        self.actor_local.train()\n",
        "        # Noise is added to bring more stability\n",
        "        if add_noise:\n",
        "            action += self.noise.sample()\n",
        "        return np.clip(action, -1, 1)\n",
        "\n",
        "    def reset(self):\n",
        "        self.noise.reset()\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        # On the basis of a batch of transitions, updates the critic and actor network\n",
        "        states, actions, rewards, next_states, dones, _, _ = experiences\n",
        "\n",
        "       # Critic network update\n",
        "        # Get predicted next-state actions and Q values from target networks and compute targets for the current states\n",
        "        actions_next = self.actor_target(next_states)\n",
        "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "        # Compute critic loss\n",
        "        Q_expected = self.critic_local(states, actions)\n",
        "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        # Minimize the loss and update the parameters\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # Actor network update\n",
        "        # Compute actor loss\n",
        "        actions_pred = self.actor_local(states)\n",
        "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
        "        # Minimize the loss and update the parameters\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Target networks updates\n",
        "        self.soft_update(self.critic_local, self.critic_target, tau)\n",
        "        self.soft_update(self.actor_local, self.actor_target, tau)                     \n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        # Target networks updates with the \\tau parameter\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
        "\n",
        "class OUNoise:\n",
        "    # Noise on action taking\n",
        "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
        "        # Initialize the parameters of the noise\n",
        "        self.mu = mu * np.ones(size)\n",
        "        self.theta = theta\n",
        "        self.sigma = sigma\n",
        "        self.seed = random.seed(seed)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = copy.copy(self.mu)\n",
        "\n",
        "    def sample(self):\n",
        "        # Returns a noise sample\n",
        "        x = self.state\n",
        "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
        "        self.state = x + dx\n",
        "        return self.state\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    # Base class\n",
        "    def __init__(self, size):\n",
        "        # Initialize a basic replay buffer\n",
        "        self._storage = []\n",
        "        self._maxsize = size\n",
        "        self._next_idx = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._storage)\n",
        "\n",
        "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
        "        data = (obs_t, action, reward, obs_tp1, done)\n",
        "        if self._next_idx >= len(self._storage):\n",
        "            self._storage.append(data)\n",
        "        else:\n",
        "            self._storage[self._next_idx] = data\n",
        "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
        "\n",
        "    def _encode_sample(self, idxes):\n",
        "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
        "        for i in idxes:\n",
        "            data = self._storage[i]\n",
        "            obs_t, action, reward, obs_tp1, done = data\n",
        "            obses_t.append(np.array(obs_t, copy=False))\n",
        "            actions.append(np.array(action, copy=False))\n",
        "            rewards.append(reward)\n",
        "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
        "            dones.append(done)\n",
        "        return np.array(obses_t), np.array(actions), np.array(rewards), np.array(obses_tp1), np.array(dones)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # Samples a batch of transitions\n",
        "        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n",
        "        return self._encode_sample(idxes)\n",
        "\n",
        "class PrioritizedReplayBuffer(ReplayBuffer):\n",
        "    def __init__(self, size, alpha):\n",
        "        \"\"\"Creates Prioritized Replay buffer.\n",
        "        Parameters\n",
        "        ----------\n",
        "        size: int\n",
        "            Max number of transitions to store in the buffer. When the buffer\n",
        "            overflows the old memories are dropped.\n",
        "        alpha: float\n",
        "            how much prioritization is used\n",
        "            (0 - no prioritization, 1 - full prioritization)\n",
        "        See Also\n",
        "        --------\n",
        "        ReplayBuffer.__init__\n",
        "        \"\"\"\n",
        "        super(PrioritizedReplayBuffer, self).__init__(size)\n",
        "        assert alpha >= 0\n",
        "        self._alpha = alpha\n",
        "\n",
        "        it_capacity = 1\n",
        "        while it_capacity < size:\n",
        "            it_capacity *= 2\n",
        "\n",
        "        self._it_sum = utils.SumSegmentTree(it_capacity)\n",
        "        self._it_min = utils.MinSegmentTree(it_capacity)\n",
        "        self._max_priority = 1.0\n",
        "\n",
        "    def add(self, *args, **kwargs):\n",
        "        \"\"\"See ReplayBuffer.store_effect\"\"\"\n",
        "        idx = self._next_idx\n",
        "        super().add(*args, **kwargs)\n",
        "        self._it_sum[idx] = self._max_priority ** self._alpha\n",
        "        self._it_min[idx] = self._max_priority ** self._alpha\n",
        "\n",
        "    def _sample_proportional(self, batch_size):\n",
        "        res = []\n",
        "        p_total = self._it_sum.sum(0, len(self._storage) - 1)\n",
        "        every_range_len = p_total / batch_size\n",
        "        for i in range(batch_size):\n",
        "            mass = random.random() * every_range_len + i * every_range_len\n",
        "            idx = self._it_sum.find_prefixsum_idx(mass)\n",
        "            res.append(idx)\n",
        "        return res\n",
        "\n",
        "    def sample(self, batch_size, beta):\n",
        "        # Samples a batch of transitions\n",
        "        assert beta > 0\n",
        "\n",
        "        idxes = self._sample_proportional(batch_size)\n",
        "\n",
        "        weights = []\n",
        "        p_min = self._it_min.min() / self._it_sum.sum()\n",
        "        max_weight = (p_min * len(self._storage)) ** (-beta)\n",
        "\n",
        "        for idx in idxes:\n",
        "            p_sample = self._it_sum[idx] / self._it_sum.sum()\n",
        "            weight = (p_sample * len(self._storage)) ** (-beta)\n",
        "            weights.append(weight / max_weight)\n",
        "        weights = np.array(weights)\n",
        "        encoded_sample = self._encode_sample(idxes)\n",
        "        states = torch.from_numpy(np.vstack([e for e in encoded_sample[0] if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e for e in encoded_sample[1] if e is not None])).float().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e for e in encoded_sample[2] if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e for e in encoded_sample[3] if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e for e in encoded_sample[4] if e is not None]).astype(np.uint8)).float().to(device)\n",
        "        \n",
        "        return tuple([states,actions,rewards,next_states,dones,weights, idxes])\n",
        "\n",
        "    def update_priorities(self, idxes, priorities):\n",
        "        \"\"\"Update priorities of sampled transitions.\n",
        "        sets priority of transition at index idxes[i] in buffer\n",
        "        to priorities[i].\n",
        "        Parameters\n",
        "        ----------\n",
        "        idxes: [int]\n",
        "            List of idxes of sampled transitions\n",
        "        priorities: [float]\n",
        "            List of updated priorities corresponding to\n",
        "            transitions at the sampled idxes denoted by\n",
        "            variable `idxes`.\n",
        "        \"\"\"\n",
        "        assert len(idxes) == len(priorities)\n",
        "        for idx, priority in zip(idxes, priorities):\n",
        "            assert priority > 0\n",
        "            assert 0 <= idx < len(self._storage)\n",
        "            self._it_sum[idx] = priority ** self._alpha\n",
        "            self._it_min[idx] = priority ** self._alpha\n",
        "\n",
        "            self._max_priority = max(self._max_priority, priority)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca1SMWg4JQlw",
        "outputId": "83027135-ebe9-45dd-fe7b-477bd94c6660"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('BipedalWalker-v3')\n",
        "env.seed(10)\n",
        "class RewardScaler(gym.RewardWrapper):\n",
        "    # Rescales the rewards given by the environment\n",
        "    def reward(self, reward):\n",
        "        return reward * 10\n",
        "agent = Agent(state_size=env.observation_space.shape[0], action_size=env.action_space.shape[0], action_space=env.action_space, random_seed=10)\n",
        "env = RewardScaler(env)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "dCYqdamRKB2V",
        "outputId": "93bf43a5-5807-4762-bdd5-944b7bd5ecc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 30\tAverage Score: -1128.28\tScore: -1181.75"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9eb6d6578743>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddpg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scores.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-9eb6d6578743>\u001b[0m in \u001b[0;36mddpg\u001b[0;34m(n_episodes, max_t)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-e608f665268f>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-e608f665268f>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences, gamma)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mQ_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mQ_targets_next\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Compute critic loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mQ_expected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_expected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Minimize the loss and update the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-77e4e8bbf59b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, a)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mlayer1_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mlayer2_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer1_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer2_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "def ddpg(n_episodes=2000, max_t=700):\n",
        "    # Initialization of a queue to easily compute running means of rewards\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        agent.reset()\n",
        "        score = 0\n",
        "        # For each time step, take an action, get the reward and learn\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break \n",
        "        scores_deque.append(score)\n",
        "        scores.append(score)\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i_episode, np.mean(scores_deque), score), end=\"\")\n",
        "        if i_episode % 100 == 0:\n",
        "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor'+str(i_episode)+'.pth')\n",
        "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic'+str(i_episode)+'.pth')\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))   \n",
        "    return scores\n",
        "\n",
        "scores = ddpg()\n",
        "np.save(\"scores.npy\", np.array(scores))\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(np.arange(1, len(scores)+1), scores)\n",
        "plt.ylabel('Reward')\n",
        "plt.xlabel('Episode')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIc0-jDaKBHa"
      },
      "outputs": [],
      "source": [
        "!pip install -U colabgymrender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S92NcBnX1ALy"
      },
      "outputs": [],
      "source": [
        "# Visualization of the agent behaviour after training\n",
        "from colabgymrender.recorder import Recorder\n",
        "env = gym.make('BipedalWalker-v3')\n",
        "env = Recorder(env, '.')\n",
        "env.seed(10)\n",
        "agent = Agent(state_size=env.observation_space.shape[0], action_size=env.action_space.shape[0], action_space=env.action_space, random_seed=10)\n",
        "\n",
        "agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
        "agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n",
        "\n",
        "state = env.reset()\n",
        "agent.reset()   \n",
        "while True:\n",
        "    action = agent.act(state)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    state = next_state\n",
        "    if done:\n",
        "        break\n",
        "        \n",
        "env.play()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "INF581_Project_PER.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
